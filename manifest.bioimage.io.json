{
  "id": "bioimage.io",
  "name": "BioImage.IO",
  "tags": [],
  "logo": "\ud83e\udd92",
  "icon": "\ud83e\udd92",
  "splash_title": "Bioimage Model Zoo",
  "splash_subtitle": "Advanced AI models in one-click",
  "splash_feature_list": [
    "Integrate with Fiji, Ilastik, ImJoy",
    "Try model instantly with BioEngine",
    "Contribute your models via Github",
    "Link models to datasets and applications"
  ],
  "explore_button_text": "Start Exploring",
  "background_image": "static/img/zoo-background.svg",
  "resource_types": [
    "model",
    "application",
    "notebook",
    "dataset"
  ],
  "default_type": "model",
  "url_root": "https://raw.githubusercontent.com/bioimage-io/bioimage-io-models/master",
  "collections": [
    {
      "id": "ilastik",
      "name": "ilastik",
      "tags": [
        "ilastik"
      ],
      "logo": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "splash_title": "ilastik",
      "splash_subtitle": "the interactive learning and segmentation toolkit",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model",
      "url_root": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master"
    },
    {
      "id": "deepimagej",
      "name": "deepImageJ",
      "tags": [
        "deepimagej"
      ],
      "logo": "https://raw.githubusercontent.com/deepimagej/models/master/logos/logo.png",
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "splash_title": "deepImageJ",
      "splash_subtitle": "A user-friendly plugin to run deep learning models in ImageJ",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/deepimagej/models/master"
    },
    {
      "id": "zero",
      "name": "ZeroCostDL4Mic",
      "version": "1.7.1",
      "tags": [
        "ZeroCostDL4Mic"
      ],
      "logo": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "icon": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "splash_title": "ZeroCostDL4Mic",
      "splash_subtitle": "A Google Colab based no-cost toolbox to explore Deep-Learning in Microscopy",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook",
        "dataset"
      ],
      "default_type": "notebook",
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "imjoy",
      "name": "ImJoy",
      "tags": [
        "imjoy"
      ],
      "logo": "https://imjoy.io/static/img/imjoy-icon.svg",
      "icon": "https://imjoy.io/static/img/imjoy-icon.svg",
      "splash_title": "ImJoy",
      "splash_subtitle": "Deep Learning Made Easy!",
      "splash_feature_list": [
        "Minimal and flexible plugin powered web application",
        "Server-less progressive web application with offline support",
        "Rich and interactive user interface powered by web technologies"
      ],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "notebook",
        "application"
      ],
      "default_type": "application",
      "url_root": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master"
    },
    {
      "id": "hpa",
      "name": "HPA",
      "tags": [
        "hpa"
      ],
      "logo": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "about_url": "https://www.proteinatlas.org/",
      "splash_title": "The Human Protein Atlas",
      "splash_subtitle": null,
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model"
    },
    {
      "id": "fiji",
      "name": "Fiji",
      "tags": [
        "fiji"
      ],
      "logo": "https://fiji.sc/site/logo.png",
      "icon": "https://fiji.sc/site/logo.png",
      "splash_title": "Fiji",
      "splash_subtitle": "Fiji is just ImageJ",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    }
  ],
  "resources": [
    {
      "id": "Notebook Preview",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/nbpreview/master/notebook-preview.imjoy.html",
      "name": "Notebook Preview",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "Previewing Jupyter notebook without a Jupyter server",
      "tags": [
        "notebook",
        "imjoy",
        "jupyter"
      ],
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "Kaibu",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/kaibu/master/Kaibu.imjoy.html",
      "name": "Kaibu",
      "version": "0.1.12",
      "api_version": "0.2.3",
      "description": "Kaibu--a web application for visualizing and annotating multi-dimensional images",
      "tags": [],
      "covers": [
        "https://raw.githubusercontent.com/imjoy-team/kaibu/master/public/static/img/kaibu-screenshot-1.png"
      ],
      "badges": [
        {
          "icon": "https://imjoy.io/static/badge/launch-imjoy-badge.svg",
          "label": "Launch ImJoy",
          "url": "https://imjoy.io/#/app?plugin=https://kaibu.org/#/app"
        },
        {
          "icon": "https://mybinder.org/badge_logo.svg",
          "label": "Launch Binder",
          "url": "https://mybinder.org/v2/gist/oeway/690c2e62311223ae93e644d542eb8949/master?filepath=Kaibu-jupyter-tutorial.ipynb"
        }
      ],
      "authors": [
        "ImJoy-Team"
      ]
    },
    {
      "id": "Ilastik",
      "type": "application",
      "source": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/src/Ilastik-app.imjoy.html",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "name": "Ilastik",
      "version": "0.1.0",
      "api_version": "0.1.7",
      "description": "Ilastik Model Preview for BioImage.io",
      "requirements": [
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"
      ],
      "dependencies": [
        "https://gist.githubusercontent.com/oeway/2d4b5899424a14d8e90ad908d4cec364/raw/TiktorchModelLoader.imjoy.html",
        "https://gist.githubusercontent.com/oeway/f09955746ec01a20053793aba83c3545/raw/CompareImages.imjoy.html"
      ],
      "env": "",
      "tags": [],
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "HPA-Classification",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/HPA-Classification.imjoy.html",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "name": "HPA-Classification",
      "version": "0.2.1",
      "api_version": "0.1.7",
      "description": "ShuffleNetV2 for HPA.",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/3.13.1/js-yaml.min.js",
        "https://cdn.jsdelivr.net/npm/apexcharts",
        "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs",
        "https://cdn.jsdelivr.net/npm/simpleheat@0.4.0/simpleheat.min.js",
        "https://cdn.jsdelivr.net/gh/photopea/UTIF.js@4f1b10cb09e244cfd4f9631245d2231537148be7/UTIF.js"
      ],
      "dependencies": [
        "https://raw.githubusercontent.com/imjoy-team/example-plugins/master/imjoy-plugins/HPA-Image-Selection.imjoy.html"
      ],
      "env": null,
      "tags": [],
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ],
      "badges": [],
      "authors": []
    },
    {
      "id": "Fiji",
      "name": "Fiji",
      "description": "Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis.",
      "source": "https://fiji.sc/",
      "cite": {
        "text": "Schindelin, J., Arganda-Carreras, I., Frise, E. et al. Fiji: an open-source platform for biological-image analysis. Nat Methods 9, 676\u2013682 (2012).",
        "doi": "https://doi.org/10.1038/nmeth.2019"
      },
      "authors": [
        "Fiji community"
      ],
      "icon": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/Fiji-icon.png",
      "documentation": "https://fiji.sc/",
      "git_repo": "https://github.com/fiji/fiji",
      "tags": [
        "fiji"
      ],
      "type": "application"
    },
    {
      "type": "notebook",
      "root_url": "https://gist.githubusercontent.com/oeway/582856630a0aed4d4d221e54df1b3ece/raw",
      "id": "vitessce-image-viewer",
      "source": "https://gist.githubusercontent.com/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/raw/vitessce-image-viewer-imjoy-demo.ipynb",
      "links": [
        "Notebook Preview"
      ],
      "name": "Vitessce Image Viewer",
      "description": "Use vitessce-image-viewer in Jupyter notebooks with ImJoy Jupyter Extension",
      "cite": null,
      "authors": [
        "Wei OUYANG"
      ],
      "badges": [
        {
          "label": "Launch Binder",
          "icon": "https://mybinder.org/badge_logo.svg",
          "url": "https://mybinder.org/v2/gist/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/master?filepath=vitessce-image-viewer-imjoy-demo.ipynb"
        },
        {
          "label": "Powered by ImJoy",
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg"
        }
      ],
      "tags": [
        "visualization",
        "imjoy"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_U-net_2D_ZeroCostDL4Mic",
      "name": "U-net (2D) - ZeroCostDL4Mic",
      "description": "U-net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-net_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "2D",
        "UNet"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-net_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "UNet2DPancreaticSegmentation",
      "source": "https://github.com/deepimagej/models/u-net_pancreatic_segmentation/U_Net_PhC-C2DL-PSC_segmentation.ipynb",
      "links": [
        "unet-pancreaticcellsegmentation"
      ],
      "name": "U-Net Pancreatic Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment phase contrast microscopy images of pancreatic stem cells on a 2D polystyrene substrate.",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal E. et al., biorXiv 2019; Ulman V. et al., Nature Methods 2017; Ronneberger O. et al., MICCAI 2015",
        "doi": null
      },
      "authors": [
        "DeepImageJ",
        "Ignacio Arganda-Carreras"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "segmentation",
        "phase contrast",
        "pancreatic stem cells",
        "deepimagej"
      ],
      "license": null,
      "format_version": null,
      "model": {
        "source": "./saved_model.pb",
        "sha256": null,
        "v1": {
          "source": "./variables",
          "sha256": null
        }
      }
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation",
      "id": "UNet2DHelaSegmentation",
      "source": "https://github.com/deepimagej/python4deepimagej/tree/master/unet",
      "name": "U-Net Hela Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment Hela cells in 2D phase contrast microscopy images",
      "cite": {
        "text": "Biomedical Imaging Group, School of Engineering, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
        "doi": null
      },
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": null,
      "tags": [
        "segmentation",
        "phase contrast",
        "hela cells",
        "deepimagej"
      ],
      "license": null,
      "format_version": null,
      "model": {
        "source": "./saved_model.pb",
        "sha256": "601830ca4462cc7b6d1047541bd2e6de105dbc92d6da42d840700eaf65aef0e7",
        "v1": {
          "source": "./variables",
          "sha256": null
        }
      }
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "UNetDA",
      "source": "src.utils.get_unet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/UNetDA.model.zip",
      "name": "U-Net DA (Domain Adaptation)",
      "description": "U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "hbp",
        "pytorch",
        "sga2",
        "ilastik",
        "brain",
        "unet2d"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/UNetCover.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "2sUNetDA",
      "source": "src.utils.get_2sunet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/2sUNetDA.model.zip",
      "name": "Two Steam U-Net DA",
      "description": "Two Steam U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). Two Steam U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. Two Steam U-Net uses differet encoders to process inputs from source and target, and use a common decoder to generate the respective segmentation outputs. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Roger Bermudez et al. A domain-adaptive two-stream U-Net for electron microscopy image segmentation. ISBI 2018.",
          "doi": "https://doi.org/10.1109/ISBI.2018.8363602"
        }
      ],
      "authors": [
        "Roger Bermudez, Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "hbp",
        "pytorch",
        "sga2",
        "ilastik",
        "brain",
        "unet2d"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/2sUNetCover.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Stardist_3D_ZeroCostDL4Mic",
      "name": "Stardist (3D) - ZeroCostDL4Mic",
      "description": "Stardist is a deep-learning method that can be used to segment cell nuclei in 3D (xyz) images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Stardist_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "Stardist",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Stardist_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Stardist_2D_ZeroCostDL4Mic_2D",
      "name": "Stardist (2D) example training and test dataset",
      "description": "Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Johanna Jukkala",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3715492",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "2D",
        "Stardist"
      ],
      "source": "https://doi.org/10.5281/zenodo.3715492",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Stardist_2D_ZeroCostDL4Mic",
      "name": "Stardist (2D) - ZeroCostDL4Mic",
      "description": "Stardist is a deep-learning method that can be used to segment cell nuclei in 2D (xy) single images or in stacks (xyz). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Stardist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "2D",
        "Stardist"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Stardist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Stardist_2D_ZeroCostDL4Mic_2D"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Noise2VOID_3D_ZeroCostDL4Mic_3D",
      "name": "Noise2VOID (3D) example training and test dataset",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713326",
      "tags": [
        "ZeroCostDL4Mic",
        "Noise2VOID",
        "denoising",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713326",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Noise2VOID_3D_ZeroCostDL4Mic",
      "name": "Noise2VOID (3D) - ZeroCostDL4Mic",
      "description": "Noise2VOID 3D is deep-learning method that can be used to denoise 3D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2VOID_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "Noise2VOID",
        "denoising",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2VOID_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2VOID_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Noise2VOID_2D_ZeroCostDL4Mic",
      "name": "Noise2VOID (2D) example training and test dataset",
      "description": "Fluorescence microscopy (paxillin-GFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Aki Stubb",
        "Guillaume Jacquemet",
        "Johanna Ivaska"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713315",
      "tags": [
        "ZeroCostDL4Mic",
        "Noise2VOID",
        "2D",
        "denoising"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713315",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Noise2VOID_2D_ZeroCostDL4Mic",
      "name": "Noise2VOID (2D) - ZeroCostDL4Mic",
      "description": "Noise2VOID 2D is deep-learning method that can be used to denoise 2D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2VOID_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "Noise2VOID",
        "2D",
        "denoising"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2VOID_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2VOID_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/v0.1.4/models/n2v-sem-demo",
      "id": "N2VSEMDemo",
      "source": "de.csbdresden.n2v.train.N2VPrediction",
      "links": [
        "Fiji"
      ],
      "download_url": "https://github.com/bioimage-io/fiji-bioimage-io/releases/download/v0.1.4/n2v-sem-demo.zip",
      "name": "N2V SEM Demo",
      "description": "Demo model for denoising trained on a single SEM image with Noise2Void",
      "cite": {
        "text": "Buchholz, T. et al. - Content-aware image restoration for electron microscopy. \nMethods in Cell Biology, Volume 152 p.277-289, ISSN 0091-679X (2019)",
        "doi": "https://doi.org/10.1016/bs.mch.2019.05.001"
      },
      "authors": [
        "Deborah Schmidt"
      ],
      "documentation": "README.md",
      "covers": [
        "thumbnail.png"
      ],
      "tags": [
        "fiji",
        "n2v",
        "denoising",
        "unet2d"
      ],
      "license": "BSD 3",
      "format_version": "0.1.0"
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_fnet_3D_ZeroCostDL4Mic",
      "name": "Label-free prediction (fnet) example training and test dataset",
      "description": "Confocal microscopy data (TOM20 labeled with Alexa Fluor 594)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Christoph Spahn"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3748967",
      "tags": [
        "labelling",
        "fnet",
        "ZeroCostDL4Mic",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3748967",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_fnet_ZeroCostDL4Mic",
      "name": "Label-free Prediction - Fnet - ZeroCostDL4Mic",
      "description": "Label-free Prediction (Fnet) is a neural network used to infer the features of cellular structures from brightfield or EM images without coloured labels. The network is trained using paired training images from the same field of view, imaged in a label-free (e.g. brightfield) and labelled condition (e.g. fluorescent protein). When trained, this allows the user to identify certain structures from brightfield images alone. The performance of fnet may depend significantly on the structure at hand. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "labelling",
        "fnet",
        "ZeroCostDL4Mic",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_fnet_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "id": "HPA-wienerschnitzelgemeinschaft",
      "name": "HPA-wienerschnitzelgemeinschaft",
      "tags": [
        "hill-climbing",
        "airnet-50",
        "inception-v3",
        "se-resnext-50",
        "airnext-50",
        "resnet-50",
        "resnet-101",
        "resnet-34",
        "classification",
        "cbam",
        "preresnet-50",
        "resnet-18",
        "hpa",
        "inception-resnet-v2"
      ],
      "authors": [
        "Shaikat Mahmood Galib",
        "Christof Henkel",
        "Kevin Hwang",
        "Dmytro Poplavskiy",
        "Bojan Tunguz",
        "Russ Wolfinger"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wienerschnitzelgemeinschaft",
      "description": "An ensemble of diverse and highly optimized CNN models, using hill climbing and weighted voting.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wienerschnitzelgemeinschaft/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "4th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-vpp",
      "name": "HPA-vpp",
      "tags": [
        "inception-v4",
        "inception-v3",
        "xception",
        "classification",
        "hpa",
        "multilayer-perceptron"
      ],
      "authors": [
        "Yinzheng Gu",
        "Chuanpeng Li",
        "Jinbin Xie"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/vpp",
      "description": "An ensemble of seven CNN models and a multi-layer perceptron network, using image augmentation, multi scales, weighted sampling and MultiLabelSoftMargin loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/vpp/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "5th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-pudae",
      "name": "HPA-pudae",
      "tags": [
        "inception-v3",
        "se-resnext-50",
        "resnet-34",
        "classification",
        "hpa"
      ],
      "authors": [
        "Park Jinmo"
      ],
      "license": "BSD-2-Clause",
      "git_repo": "https://github.com/CellProfiling/pudae-kaggle-hpa",
      "description": "An ensemble using focal loss, per image normalization and spatial attention.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/pudae-kaggle-hpa/master/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "3rd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-conv",
      "name": "HPA-conv is all you need",
      "tags": [
        "inception-v3",
        "se-resnext-50",
        "xception",
        "classification",
        "hpa"
      ],
      "authors": [
        "Xuan Cao",
        "Runmin Wei",
        "Yuanhao Wu",
        "Xun Zhu"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/conv_is_all_you_need",
      "description": "An ensemble of a cropping window CNN based on Xception, and two conventional CNNs based on SE-ResNext50 and InceptionV3.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/conv_is_all_you_need/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "10th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-bestfitting",
      "name": "HPA-bestfitting",
      "tags": [
        "inception-v3",
        "resnet-50",
        "resnet-34",
        "classification",
        "hpa",
        "densenet-121"
      ],
      "authors": [
        "Shubin Dai"
      ],
      "license": "MIT",
      "cite": null,
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting",
      "description": "A CNN model using focal loss and image augmentation, optimized with Adam optimizer.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "1st",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png",
        "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/src/bestfitting-densenet-diagram.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-WAIR",
      "name": "HPA-WAIR",
      "tags": [
        "opencv",
        "densenet-169",
        "se-resnext-50",
        "xception",
        "classification",
        "hpa",
        "scikit-learn",
        "densenet-121",
        "ibn-densenet-121"
      ],
      "authors": [
        "Jun Lan"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wair",
      "description": "Seven CNN models, using multiple different architectures, ensembled through averaging.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wair/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "2nd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-Random-Walk",
      "name": "HPA-Random Walk",
      "tags": [
        "hpa",
        "classification",
        "attention-gate",
        "resnet-18"
      ],
      "authors": [
        "Zhifeng Gao",
        "Cheng Ju",
        "Xiaohan Yi",
        "Hongdong Zheng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/random_walk",
      "description": "An ensemble model of three different resolutions based on single attention gated network.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/random_walk/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "38th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-One-More",
      "name": "HPA-One More Layer Of Stacking",
      "tags": [
        "hpa",
        "inception-v4",
        "se-resnext-50",
        "xception",
        "classification",
        "bn-inception",
        "lightgbm"
      ],
      "authors": [
        "Dmitry Buslov",
        "Sergei Fironov",
        "Alexander Kiselev",
        "Dmytro Panchenko"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/one_more_layer_of_stacking",
      "description": "14 CNN models ensembled via LightGBM stacking, optimized with Wadam, using focal and LSEP loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/one_more_layer_of_stacking/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "8th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-NTU_MiRA",
      "name": "HPA-NTU_MiRA",
      "tags": [
        "resnet-34",
        "hpa",
        "classification"
      ],
      "authors": [
        "Kuan-Lun Tseng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/ntu_mira",
      "description": "A CNN with large input size (1024 \u2a09 1024 px) using fixed batch-normalization and data distillation.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/ntu_mira/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "16th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/models/HPAShuffleNetV2",
      "id": "HPAShuffleNetV2",
      "source": "https://raw.githubusercontent.com/oeway/Anet-Model-Zoo/master/4_hpa_shufflenet_v2/model.json",
      "links": [
        "HPA-Classification"
      ],
      "name": "HPA ShuffleNetV2",
      "description": "A light-weight model for HPA image classification competition",
      "cite": null,
      "authors": [
        "Moshe Livne",
        "Wei OUYANG"
      ],
      "documentation": "HPAShuffleNetV2.md",
      "tags": [
        "shufflenet",
        "tensorflow.js",
        "classification",
        "imjoy",
        "hpa"
      ],
      "git_repo": "https://github.com/CellProfiling/HPA-Special-Prize",
      "badges": [
        {
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg",
          "label": "Powered by ImJoy"
        }
      ],
      "license": "Apache 2.0",
      "format_version": "0.1.0",
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": [
          {
            "id": "v1",
            "name": "version 1",
            "description": "weights trained for segmenting small extracellular vesicles",
            "source": "./variables",
            "sha256": null
          }
        ]
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation",
      "id": "FRUNet2DsEVSegmentation",
      "source": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "download_url": "https://github.com/deepimagej/models/releases/download/0.1/fru-net_sev_segmentation.zip",
      "name": "Fully Residual U-Net - TEM",
      "description": "DeepImageJ compatible fully residual U-Net trained to segment small extracellular vesicles in 2D TEM images",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal, E. et al., Deep-Learning-Based Segmentation of SmallExtracellular Vesicles in Transmission Electron Microscopy Images \nScientific Reports, (2019)",
        "doi": "https://doi.org/10.1038/s41598-019-49431-3"
      },
      "authors": [
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Martin Ma\u0161ka",
        "Anna Kotrbov\u00e1",
        "Vendula Posp\u00edchalov\u00e1",
        "Pavel Matula and Arrate Mu\u00f1oz-Barrutia"
      ],
      "documentation": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "covers": [
        "frunet_sev.jpg"
      ],
      "tags": [
        "extracellular vesicles",
        "segmentation",
        "TEM",
        "deepimagej"
      ],
      "license": "BSD 3",
      "format_version": "0.2.0",
      "model": {
        "source": "./saved_model.pb",
        "sha256": "9ccb79070f30813e7447342e3ab7f4107a314a1414c1541c79134ef950ac4a7f"
      }
    },
    {
      "type": "dataset",
      "attachments": {
        "files": [
          "https://gist.githubusercontent.com/manzt/d16dbac0ea3adc3c7b9b61f54fa1f78d/raw/95854058512862accb0182d4a02f86a55ad19139/"
        ]
      },
      "root_url": "https://gist.githubusercontent.com/oeway/3dcbd79cd29da42be13a7e3bc0f9ca12/raw",
      "id": "dummy_zarr",
      "source": "https://gist.githubusercontent.com/oeway/3dcbd79cd29da42be13a7e3bc0f9ca12/raw/dummy_zarr.dataset.yaml",
      "name": "Dummy Zarr",
      "description": "A dummy zarr dataset",
      "cite": null,
      "authors": [
        "NO ONE"
      ],
      "documentation": null,
      "tags": [
        "zarr"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) example training and test dataset",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713337",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713337",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) example training and test dataset",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713330",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "2D",
        "denoising"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713330",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, Seamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Lo\u00efc Alain Royer, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "2D",
        "denoising"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/platybrowser/platybrowser/3711f1c26e5db8c38c3faff4cccb3110560e3c67/segmentation/cells/UNet3DPlatyCellProbs.model",
      "id": "UNet3DPlatyCellProbs",
      "source": "mmpb.segmentation.network.models.UNetAnisotropic",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/platybrowser/platybrowser/releases/download/1.0.0/UNet3DPlatyCellProbs.model.zip",
      "name": "3D UNet Platynereis Cell Segmentation (Probabilities)",
      "description": "A 3d U-Net trained to predict the cell boundaries in a EM volume of a 6 day old Platynereis.",
      "cite": [
        {
          "text": "Vergara, Hernando M. et al. Whole-body integration of gene expression and single-cell morphology. BioRxiv 2020.\"",
          "doi": "https://doi.org/10.1101/2020.02.26.961037"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "cell membrane",
        "pytorch",
        "unet3d",
        "ilastik",
        "segmentation",
        "EM",
        "platynereis"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_raw.png",
        "ilastik_pred.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/wolny/pytorch-3dunet/37f186c80f4d64b1dab5d165d8c2aae15b5aede1/bioimage-io/UNet3DArabidopsisOvules.model",
      "id": "UNet3DArabidopsisOvules",
      "source": "pytorch3dunet.unet3d.model.UNet3D",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/wolny/pytorch-3dunet/releases/download/1.2.6/UNet3DArabidopsisOvules.model.zip",
      "name": "3D UNet Arabidopsis Ovules",
      "description": "A 3d U-Net trained to predict the cell boundaries in confocal stacks of Arabidopsis ovules. Voxel size: (0.235, 0.150, 0.150) microns ZYX",
      "cite": [
        {
          "text": "Wolny, Adrian et al. Accurate and Versatile 3D Segmentation of Plant Tissues at Cellular Resolution. BioRxiv 2020.",
          "doi": "https://doi.org/10.1101/2020.01.17.910562"
        }
      ],
      "authors": [
        "Adrian Wolny;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "cell membrane",
        "arabidopsis",
        "plant tissue",
        "pytorch",
        "ovuls",
        "unet3d",
        "ilastik",
        "segmentation"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_4.png",
        "ilastik_5.png",
        "ilastik_6.png",
        "ilastik_7.png",
        "ilastik_8.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/pytorch-bioimage-io/v0.1.1/specs/models/unet2d/nuclei_broad",
      "id": "UNet2DNucleiBroad",
      "source": "pybio.torch.models.unet.UNet2d",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/bioimage-io/pytorch-bioimage-io/releases/download/v0.1.1/UNet2DNucleiBroad.model.zip",
      "name": "2D UNet Nuclei Broad",
      "description": "A 2d U-Net pretrained on broad nucleus dataset.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io",
        "Fynn Beuttenm\u00fcller"
      ],
      "documentation": "UNet2DNucleiBroad.md",
      "tags": [
        "ilastik",
        "pytorch",
        "nucleus-segmentation",
        "unet2d"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "cover0.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "unet-pancreaticcellsegmentation",
      "name": "2D U-Net for binary segmentation",
      "description": "Easy example to define a 2D U-Net for segmentation with Keras and import it into DeepImageJ format",
      "cite": {
        "text": "Falk, T., Mai, D., Bensch, R. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods 16, 67\u201370 (2019).",
        "doi": "https://doi.org/10.1038/s41592-018-0261-2"
      },
      "authors": [
        "Ignacio Arganda-Carreras and DeepImageJ"
      ],
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/notebook_intro.png",
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/usecase.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/deepimagej/models/blob/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
        }
      ],
      "documentation": "https://github.com/miura/NEUBIAS_AnalystSchool2020/tree/master/Ignacio",
      "tags": [
        "segmentation",
        "DeepImageJ",
        "UNet",
        "deepimagej"
      ],
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
    }
  ]
}