{
  "id": "bioimage.io",
  "name": "BioImage.IO",
  "tags": [],
  "logo": "\ud83e\udd92",
  "icon": "\ud83e\udd92",
  "splash_title": "Bioimage Model Zoo",
  "splash_subtitle": "Advanced AI models in one-click",
  "splash_feature_list": [
    "Integrate with Fiji, Ilastik, ImJoy",
    "Try model instantly with BioEngine",
    "Contribute your models via Github",
    "Link models to datasets and applications"
  ],
  "explore_button_text": "Start Exploring",
  "background_image": "static/img/zoo-background.svg",
  "resource_types": [
    "model",
    "application",
    "notebook",
    "dataset"
  ],
  "default_type": "model",
  "url_root": "https://raw.githubusercontent.com/bioimage-io/bioimage-io-models/master",
  "collections": [
    {
      "id": "ilastik",
      "name": "ilastik",
      "tags": [
        "ilastik"
      ],
      "logo": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "splash_title": "ilastik",
      "splash_subtitle": "the interactive learning and segmentation toolkit",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model",
      "url_root": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master"
    },
    {
      "id": "zero",
      "name": "ZeroCostDL4Mic",
      "version": "1.7.1",
      "tags": [
        "ZeroCostDL4Mic"
      ],
      "logo": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "icon": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "splash_title": "ZeroCostDL4Mic",
      "splash_subtitle": "A Google Colab based no-cost toolbox to explore Deep-Learning in Microscopy",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook",
        "dataset"
      ],
      "default_type": "notebook",
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "imjoy",
      "name": "ImJoy",
      "tags": [
        "imjoy"
      ],
      "logo": "https://imjoy.io/static/img/imjoy-icon.svg",
      "icon": "https://imjoy.io/static/img/imjoy-icon.svg",
      "splash_title": "ImJoy",
      "splash_subtitle": "Deep Learning Made Easy!",
      "splash_feature_list": [
        "Minimal and flexible plugin powered web application",
        "Server-less progressive web application with offline support",
        "Rich and interactive user interface powered by web technologies"
      ],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "notebook",
        "application"
      ],
      "default_type": "application",
      "url_root": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master"
    },
    {
      "id": "hpa",
      "name": "HPA",
      "tags": [
        "hpa"
      ],
      "logo": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "about_url": "https://www.proteinatlas.org/",
      "splash_title": "The Human Protein Atlas",
      "splash_subtitle": null,
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model"
    },
    {
      "id": "fiji",
      "name": "Fiji",
      "tags": [
        "fiji"
      ],
      "logo": "https://fiji.sc/site/logo.png",
      "icon": "https://fiji.sc/site/logo.png",
      "splash_title": "Fiji",
      "splash_subtitle": "Fiji is just ImageJ",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "deepimagej",
      "name": "DeepImageJ",
      "tags": [
        "deepimagej"
      ],
      "logo": "https://raw.githubusercontent.com/deepimagej/models/master/logos/logo.png",
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "splash_title": "deepImageJ",
      "splash_subtitle": "A user-friendly plugin to run deep learning models in ImageJ",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook",
        "application"
      ],
      "url_root": "https://raw.githubusercontent.com/deepimagej/models/master"
    }
  ],
  "resources": [
    {
      "id": "vizarr",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/vizarr.imjoy.html",
      "icon": "extension",
      "name": "vizarr",
      "version": "0.0.1",
      "api_version": "0.1.8",
      "description": "A minimal, purely client-side program for viewing Zarr-based images with Viv & ImJoy",
      "requirements": [],
      "dependencies": [],
      "env": "",
      "tags": [],
      "documentation": "https://raw.githubusercontent.com/hms-dbmi/vizarr/master/README.md",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "Notebook Preview",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/nbpreview/master/notebook-preview.imjoy.html",
      "name": "Notebook Preview",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "Previewing Jupyter notebook without a Jupyter server",
      "tags": [
        "jupyter",
        "notebook",
        "imjoy"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "Kaibu",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/kaibu/master/Kaibu.imjoy.html",
      "name": "Kaibu",
      "version": "0.1.12",
      "api_version": "0.2.3",
      "description": "Kaibu--a web application for visualizing and annotating multi-dimensional images",
      "tags": [],
      "documentation": null,
      "covers": [
        "https://raw.githubusercontent.com/imjoy-team/kaibu/master/public/static/img/kaibu-screenshot-1.png"
      ],
      "badges": [
        {
          "icon": "https://imjoy.io/static/badge/launch-imjoy-badge.svg",
          "label": "Launch ImJoy",
          "url": "https://imjoy.io/#/app?plugin=https://kaibu.org/#/app"
        },
        {
          "icon": "https://mybinder.org/badge_logo.svg",
          "label": "Launch Binder",
          "url": "https://mybinder.org/v2/gist/oeway/690c2e62311223ae93e644d542eb8949/master?filepath=Kaibu-jupyter-tutorial.ipynb"
        }
      ],
      "authors": [
        "ImJoy-Team"
      ]
    },
    {
      "id": "ImageJ.JS",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/imagej-js.imjoy.html",
      "name": "ImageJ.JS",
      "version": "0.1.6",
      "api_version": "0.2.3",
      "description": "ImageJ running in the browser",
      "tags": [],
      "documentation": "https://raw.githubusercontent.com/imjoy-team/imagej.js/master/README.md",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "ImJoyFiddle",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/imjoy-fiddle.imjoy.html",
      "name": "ImJoyFiddle",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "ImJoyFiddle -- a playground for ImJoy plugins",
      "tags": [],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "Ilastik",
      "type": "application",
      "source": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/src/Ilastik-app.imjoy.html",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "name": "Ilastik",
      "version": "0.1.1",
      "api_version": "0.1.7",
      "description": "Ilastik Model Preview for BioImage.io",
      "requirements": [
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"
      ],
      "dependencies": [
        "https://gist.githubusercontent.com/oeway/2d4b5899424a14d8e90ad908d4cec364/raw/TiktorchModelLoader.imjoy.html",
        "https://gist.githubusercontent.com/oeway/f09955746ec01a20053793aba83c3545/raw/CompareImages.imjoy.html"
      ],
      "env": "",
      "tags": [],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "HPA-Classification",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/HPA-Classification.imjoy.html",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "name": "HPA-Classification",
      "version": "0.2.1",
      "api_version": "0.1.7",
      "description": "ShuffleNetV2 for HPA.",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/3.13.1/js-yaml.min.js",
        "https://cdn.jsdelivr.net/npm/apexcharts",
        "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs",
        "https://cdn.jsdelivr.net/npm/simpleheat@0.4.0/simpleheat.min.js",
        "https://cdn.jsdelivr.net/gh/photopea/UTIF.js@4f1b10cb09e244cfd4f9631245d2231537148be7/UTIF.js"
      ],
      "dependencies": [
        "https://raw.githubusercontent.com/imjoy-team/example-plugins/master/imjoy-plugins/HPA-Image-Selection.imjoy.html"
      ],
      "env": null,
      "tags": [],
      "documentation": null,
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ],
      "badges": [],
      "authors": []
    },
    {
      "id": "Fiji",
      "name": "Fiji",
      "description": "Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis.",
      "source": "https://fiji.sc/",
      "cite": {
        "text": "Schindelin, J., Arganda-Carreras, I., Frise, E. et al. Fiji: an open-source platform for biological-image analysis. Nat Methods 9, 676\u2013682 (2012).",
        "doi": "https://doi.org/10.1038/nmeth.2019"
      },
      "authors": [
        "Fiji community"
      ],
      "icon": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/Fiji-icon.png",
      "documentation": "https://fiji.sc/",
      "git_repo": "https://github.com/fiji/fiji",
      "tags": [
        "fiji"
      ],
      "type": "application"
    },
    {
      "id": "deepimagej",
      "type": "application",
      "name": "DeepImageJ",
      "description": "DeepImageJ is a user-friendly plugin that enables the use of pre-trained deep learning models in ImageJ and Fiji.",
      "source": "https://deepimagej.github.io/deepimagej/index.html",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal, E., Garc\u00eda-L\u00f3pez-de-Haro, C., Donati, L., Unser, M., Mu\u00f1oz-Barrutia, A. and Sage, D. DeepImageJ: A user-friendly plugin to run deep learning models in ImageJ, BioRxiv, 2019",
        "doi": "https://doi.org/10.1101/799270"
      },
      "authors": [
        "DeepImageJ team"
      ],
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "documentation": "https://deepimagej.github.io/deepimagej/index.html",
      "git_repo": "https://github.com/deepimagej/deepimagej-plugin",
      "tags": [
        "deepimagej"
      ]
    },
    {
      "id": "CellPose-ImageJ",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/cellpose-imagej.imjoy.html",
      "name": "CellPose-ImageJ",
      "version": "0.1.6",
      "api_version": "0.2.3",
      "description": "Running CellPose segmentation in ImageJ.JS",
      "tags": [],
      "documentation": "https://raw.githubusercontent.com/imjoy-team/imagej.js/master/README.md",
      "covers": [
        "https://dl.dropbox.com/s/09ch005yiasykve/imjoy-cellpose-imagej.gif"
      ],
      "badges": [],
      "authors": []
    },
    {
      "id": "CellPose",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/cellpose.imjoy.html",
      "name": "CellPose",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "a generalist algorithm for cellular segmentation",
      "tags": [],
      "documentation": "https://cellpose.readthedocs.io/en/latest/",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "type": "dataset",
      "id": "Dataset_pix2pix_ZeroCostDL4Mic",
      "name": "pix2pix example training and test dataset - ZeroCostDL4Mic",
      "description": "Paired microscopy images (fluorescence) of lifeact-RFP and sir-DNA",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941889",
      "tags": [
        "ZeroCostDL4Mic",
        "pix2pix"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941889",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/paired-image_translation.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_pix2pix_2D_ZeroCostDL4Mic",
      "name": "pix2pix (2D) - ZeroCostDL4Mic",
      "description": "pix2pix is a deep-learning method that can be used to translate one type of images into another. While pix2pix can potentially be used for any type of image-to-image translation, we demonstrate that it can be used to predict a fluorescent image from another fluorescent image. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "pix2pix",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_pix2pix_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_YOLOv2_ZeroCostDL4Mic",
      "name": "YoloV2 example training and test dataset - ZeroCostDL4Mic",
      "description": "2D grayscale .png images with corresponding bounding box annotations in .xml  PASCAL Voc format.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet",
        "Lucas von Chamier"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941908",
      "tags": [
        "ZeroCostDL4Mic",
        "YOLOv2"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941908",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4290871/files/txred_tensorflow_saved_model_bundle.zip",
            "sha256": "5aceebca9742c0239de62e38013a223c811cc87e582406b935011eeb92464fe9",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_txred_super-resolution",
      "id": "WidefieldTxredSuperResolution",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_txred_super-resolution/model.yaml",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_txred_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Widefield TxRed super-resolution",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "deepimagej",
        "Fluorescence microscopy",
        "GAN"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": null
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4290871/files/fitc_tensorflow_saved_model_bundle.zip?download=1",
            "sha256": "df70826cfe40306f3631360260fd2c520e763d4a130700de05d85e79cdc5a553",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_fitc_super-resolution",
      "id": "WidefieldFitcSuperResolution",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_fitc_super-resolution/model.yaml",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_fitc_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Widefield FITC super-resolution",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "deepimagej",
        "Fluorescence microscopy",
        "GAN"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": null
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4290871/files/txred_tensorflow_saved_model_bundle.zip",
            "sha256": "68f57fa211524fa258c3212f04e67c668a73c757033f3d0fbca69c2b1539c7cf",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_dapi_super-resolution",
      "id": "WidefieldDapiSuperResolution",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_dapi_super-resolution/model.yaml",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_dapi_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Widefield DAPI super-resolution",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "deepimagej",
        "Fluorescence microscopy",
        "GAN"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": null
    },
    {
      "type": "notebook",
      "root_url": "https://gist.githubusercontent.com/oeway/582856630a0aed4d4d221e54df1b3ece/raw",
      "id": "vitessce-image-viewer",
      "source": "https://gist.githubusercontent.com/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/raw/vitessce-image-viewer-imjoy-demo.ipynb",
      "links": [
        "Notebook Preview"
      ],
      "name": "Vitessce Image Viewer",
      "description": "Use vitessce-image-viewer in Jupyter notebooks with ImJoy Jupyter Extension",
      "cite": null,
      "authors": [
        "Wei OUYANG"
      ],
      "badges": [
        {
          "label": "Launch Binder",
          "icon": "https://mybinder.org/badge_logo.svg",
          "url": "https://mybinder.org/v2/gist/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/master?filepath=vitessce-image-viewer-imjoy-demo.ipynb"
        },
        {
          "label": "Powered by ImJoy",
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg"
        }
      ],
      "tags": [
        "visualization",
        "imjoy"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://github.com/deepimagej/models/releases/download/0.3/u-net_pancreatic_cell_segmentation_tf_model.zip",
            "sha256": "c1a4a551af6246e9f998c919ac675d437ee1ddfd9c1bd4004b083d9e736b2a43",
            "test_inputs": [
              "./exampleImage.tiff"
            ],
            "test_outputs": [
              "./resultImage.tiff"
            ],
            "documentation": "https://github.com/deepimagej/models/u-net_pancreatic_segmentation/U_Net_PhC-C2DL-PSC_segmentation.ipynb"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "UNet2DPancreaticSegmentation",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/model.yaml",
      "download_url": "https://github.com/deepimagej/models/releases/download/0.3/u-net_pancreatic_segmentation.zip",
      "links": [
        "deepimagej",
        "unet-pancreaticcellsegmentation"
      ],
      "format_version": "0.3.0",
      "name": "U-Net Pancreatic Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment phase contrast microscopy images of pancreatic stem cells on a 2D polystyrene substrate.",
      "cite": [
        {
          "text": "G\u00f3mez-de-Mariscal E. et al., biorXiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        },
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.4473"
        },
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Ignacio Arganda-Carreras",
        "DeepImageJ team"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "deepimagej",
        "pancreatic stem cells",
        "phase contrast",
        "segmentation"
      ],
      "license": "BSD-2"
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4155785/files/u-net_hela_segmentation_tf_model.zip",
            "sha256": "1a64fcdbd0835c2b490bb2fdfe33e3321c022a7c5dd82c7b99661c808175a559",
            "test_input": "./exampleImage.tiff",
            "test_output": "./resultImage.tiff"
          },
          "tensorflow_js": {
            "source": "https://raw.githubusercontent.com/deepimagej/tensorflow-js-models/main/u-net_hela_segmentation_tf_js_model/model.json",
            "sha256": "816f39edcc4a16aa60e8ea485c9eac31dc54181241252270ee5e2ae0bb71b49d",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation",
      "id": "UNet2DHelaSegmentation",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation/model.yaml",
      "links": [
        "deepimagej"
      ],
      "download_url": "https://zenodo.org/record/4155785/files/deepimagej_u-net_hela_segmentation.zip",
      "format_version": "0.3.0",
      "name": "U-Net Hela Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment Hela cells in 2D phase contrast microscopy images",
      "cite": [
        {
          "text": "Biomedical Imaging Group, School of Engineering, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
          "doi": null
        }
      ],
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "unet_hela_seg.jpg"
      ],
      "tags": [
        "deepimagej",
        "phase contrast",
        "hela cells",
        "segmentation"
      ],
      "license": "BSD-2",
      "git_repo": "https://github.com/deepimagej/python4deepimagej/tree/master/unet"
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_protobuffer": {
            "source": "https://zenodo.org/record/4155785/files/u-net_glioblastoma_segmentation_tf_model.zip",
            "sha256": "b72412b1457c9f3e37850914308381c7c81910e04b2edc7697bb0293c15b57d4",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          },
          "tensorflow_js": {
            "source": "https://raw.githubusercontent.com/deepimagej/tensorflow-js-models/main/u-net_glioblastoma_segmentation_tf_js_model/model.json",
            "sha256": "34a8002c671a8d64c6eee1395df921e8d55daa60ad843e07c5c62c9b6653c6cd",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_glioblastoma_segmentation",
      "id": "UNet2DGlioblastomaSegmentation",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_glioblastoma_segmentation/model.yaml",
      "download_url": "https://zenodo.org/record/4155785/files/deepimagej_u-net_glioblastoma_segmentation.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "U-Net GLioblastoma Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment 2D phase contrast microscopy images of glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate.",
      "cite": [
        {
          "text": "Biomedical Imaging Group, School of Engineering, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
          "doi": null
        }
      ],
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "cover_image.jpg"
      ],
      "tags": [
        "deepimagej",
        "phase contrast",
        "glioblastoma cells",
        "segmentation"
      ],
      "license": "BSD-2",
      "git_repo": "https://github.com/deepimagej/python4deepimagej/tree/master/unet"
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "UNetDA",
      "source": "src.utils.get_unet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/UNetDA.model.zip",
      "name": "U-Net DA (Domain Adaptation)",
      "description": "U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "unet2d",
        "hbp",
        "ilastik",
        "pytorch",
        "sga2",
        "brain"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/UNetCover.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_U-Net_3D_ZeroCostDL4Mic",
      "name": "U-Net (3D) - ZeroCostDL4Mic",
      "description": "The 3D U-Net was first introduced by \u00c7i\u00e7ek et al for learning dense volumetric segmentations from sparsely annotated ground-truth data building upon the original U-Net architecture by Ronneberger et al. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Daniel Krentzel and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "3D",
        "U-Net",
        "ZeroCostDL4Mic",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_U-Net_2D_ZeroCostDL4Mic_DeepImageJ",
      "name": "U-Net (2D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "U-Net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-Net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via DeepImageJ.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the DeepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/U-Net_2D_ZeroCostDL4Mic_exportDeepImageJ.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "U-Net",
        "ZeroCostDL4Mic",
        "2D",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/U-Net_2D_ZeroCostDL4Mic_exportDeepImageJ.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_U-Net_2D_ZeroCostDL4Mic",
      "name": "U-Net (2D) - ZeroCostDL4Mic",
      "description": "U-Net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-Net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "U-Net",
        "ZeroCostDL4Mic",
        "2D",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "2sUNetDA",
      "source": "src.utils.get_2sunet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/2sUNetDA.model.zip",
      "name": "Two Steam U-Net DA",
      "description": "Two Steam U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). Two Steam U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. Two Steam U-Net uses differet encoders to process inputs from source and target, and use a common decoder to generate the respective segmentation outputs. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Roger Bermudez et al. A domain-adaptive two-stream U-Net for electron microscopy image segmentation. ISBI 2018.",
          "doi": "https://doi.org/10.1109/ISBI.2018.8363602"
        }
      ],
      "authors": [
        "Roger Bermudez, Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "unet2d",
        "hbp",
        "ilastik",
        "pytorch",
        "sga2",
        "brain"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/2sUNetCover.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_StarDist_3D_ZeroCostDL4Mic",
      "name": "StarDist (3D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 3D (xyz) images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "3D",
        "ZeroCostDL4Mic",
        "StarDist",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
      "name": "StarDist (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Johanna Jukkala",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3715492",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist",
        "2D",
        "segmentation"
      ],
      "source": "https://doi.org/10.5281/zenodo.3715492",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Stardist_nuclei_masks.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_StarDist_2D_ZeroCostDL4Mic",
      "name": "StarDist (2D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 2D (xy) single images or in stacks (xyz). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist",
        "2D",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
        "Dataset_StarDist_Fluo_ZeroCostDL4Mic",
        "Dataset_StarDist_brightfield_ZeroCostDL4Mic",
        "Dataset_StarDist_brightfield2_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_SplineDist_2D_ZeroCostDL4Mic",
      "name": "SplineDist (2D) - ZeroCostDL4Mic",
      "description": "SplineDist is a neural network inspired by StarDist, capable of performing image instance segmentation. Unlike StarDist, SplineDist uses cubic splines to describe the contour of each object and therefore can potentially segment objects of any shapes. This version is only for 2D dataset. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain F. Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/SplineDist_overlay_cropped.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/SplineDist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist",
        "SplineDist",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/SplineDist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_RCAN_3D_ZeroCostDL4Mic",
      "name": "RCAN (3D) - ZeroCostDL4Mic",
      "description": "RCAN is a neural network capable of image restoration from corrupted bio-images. The network allows image denoising and resolution improvement in 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/3D-RCAN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/3D-RCAN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2Void (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713326",
      "tags": [
        "3D",
        "ZeroCostDL4Mic",
        "denoising",
        "Noise2Void"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713326",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (paxillin-GFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Aki Stubb",
        "Guillaume Jacquemet",
        "Johanna Ivaska"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713315",
      "tags": [
        "ZeroCostDL4Mic",
        "denoising",
        "2D",
        "Noise2Void"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713315",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/N2V_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) - ZeroCostDL4Mic",
      "description": "Noise2Void 2D is deep-learning method that can be used to denoise 2D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "denoising",
        "Noise2VOID",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2Void_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2VOID (3D) - ZeroCostDL4Mic",
      "description": "Noise2VOID 3D is deep-learning method that can be used to denoise 3D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "3D",
        "ZeroCostDL4Mic",
        "denoising",
        "Noise2Void"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2Void_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/v0.1.4/models/n2v-sem-demo",
      "id": "N2VSEMDemo",
      "source": "de.csbdresden.n2v.train.N2VPrediction",
      "links": [
        "Fiji"
      ],
      "download_url": "https://github.com/bioimage-io/fiji-bioimage-io/releases/download/v0.1.4/n2v-sem-demo.zip",
      "name": "N2V SEM Demo",
      "description": "Demo model for denoising trained on a single SEM image with Noise2Void",
      "cite": {
        "text": "Buchholz, T. et al. - Content-aware image restoration for electron microscopy. \nMethods in Cell Biology, Volume 152 p.277-289, ISSN 0091-679X (2019)",
        "doi": "https://doi.org/10.1016/bs.mch.2019.05.001"
      },
      "authors": [
        "Deborah Schmidt"
      ],
      "documentation": "README.md",
      "covers": [
        "thumbnail.png"
      ],
      "tags": [
        "denoising",
        "fiji",
        "unet2d",
        "n2v"
      ],
      "license": "BSD 3",
      "format_version": "0.1.0"
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4290839/files/mt3_vstain_tensorflow_saved_model_bundle.zip",
            "sha256": "9c20009f5eb0f751dcd4b9f1d43ef7b7ec989b41d6edd080804595aa436b6201",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/mt3_virtual_staining",
      "id": "Mt3VirtualStaining",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/mt3_virtual_staining/model.yaml",
      "download_url": "https://zenodo.org/record/4290839/files/mt3_virtual_staining.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Masson\u2019s trichrome (MT3) virtual staining",
      "description": "A trained GAN to transform wide-field autofluorescence images of unlabelled tissue sections into images that are equivalent to the bright-field images of histologically stained versions of the same samples.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Biomedical Engineering, 2019",
          "doi": "https://doi.org/10.1038/s41551-019-0362-y"
        }
      ],
      "authors": [
        "Yair Rivenson",
        "Hongda Wang",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "deepimagej",
        "Virtual staining",
        "GAN",
        "histology"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": null
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4155785/files/MU-Lux_CTC_PhC-C2DL-PSC_tf_saved_model_bundle.zip",
            "sha256": "2f436133d4373a4080b8412985861b90be03166c5d07f477304d53b30d8cc28b",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          },
          "keras_hdf5": {
            "source": "https://zenodo.org/record/4155785/files/MU-Lux_CTC_unet_PhC-C2DL-PSC.h5",
            "sha256": "0cc15900861ed5e5e7a6254400a85ed6f9b5d488bf1204d1274d36386b239283",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/mu-lux_ctc_phc-c2dl-psc",
      "id": "MU-Lux_CTC_PhC-C2DL-PSC",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/mu-lux_ctc_phc-c2dl-psc/model.yaml",
      "download_url": "https://zenodo.org/record/4155785/files/MU-Lux_CTC_PhC-C2DL-PSC.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "MU-Lux-CZ CTC submission - PhC-C2DL-PSC dataset",
      "description": "The method combines deep learning with watershed segmentation. For each frame, the convolutional neural network of U-Net shape detects all cells by markers and recognizes the foreground and the background of the frame. Then, the final segmentation is generated by a Marker-Controlled Watershed transformation.",
      "cite": [
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.447"
        },
        {
          "text": "Filip Lux and Petr Matula, arXiv 2020",
          "doi": "https://arxiv.org/abs/2004.01607"
        }
      ],
      "authors": [
        "Filip Lux, Centre for Biomedical Image Analysis, Masaryk University",
        "Petr Matula, Centre for Biomedical Image Analysis, Masaryk University"
      ],
      "git_repo": "https://gitlab.fi.muni.cz/xlux/deepwater",
      "documentation": "http://public.celltrackingchallenge.net/participants/MU-Lux-CZ.pdf",
      "covers": [
        "cover.jpg"
      ],
      "tags": [
        "watershed",
        "phase contrast",
        "deepwater",
        "cell tracking challenge",
        "deepimagej",
        "segmentation"
      ],
      "license": null
    },
    {
      "type": "dataset",
      "id": "LuCa-7color",
      "name": "LuCa-7color",
      "description": "Sample PerkinElmer Vectra QPTIFF files (c) PerkinElmer (http://www.perkinelmer.com)",
      "authors": [
        "Perkin Elmer"
      ],
      "tags": [
        "OME-TIFF",
        "imjoy"
      ],
      "license": "CC-BY 4.0",
      "download_url": "https://downloads.openmicroscopy.org/images/Vectra-QPTIFF/perkinelmer/PKI_scans/LuCa-7color_Scan1.qptiff",
      "source": "https://viv-demo.storage.googleapis.com/LuCa-7color_Scan1/data.zarr/0",
      "covers": [
        "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/asset/LuCa-7color_Scan1.png"
      ],
      "links": [
        "vizarr"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_fnet_3D_ZeroCostDL4Mic",
      "name": "Label-free prediction (fnet) example training and test dataset - ZeroCostDL4Mic",
      "description": "Confocal microscopy data (TOM20 labeled with Alexa Fluor 594)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Christoph Spahn"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3748967",
      "tags": [
        "3D",
        "ZeroCostDL4Mic",
        "labelling",
        "fnet"
      ],
      "source": "https://doi.org/10.5281/zenodo.3748967",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Fnet_exemplary_data_mitochondria.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_fnet_ZeroCostDL4Mic",
      "name": "Label-free Prediction - fnet - (3D) ZeroCostDL4Mic",
      "description": "Label-free Prediction (fnet) is a neural network used to infer the features of cellular structures from brightfield or EM images without coloured labels. The network is trained using paired training images from the same field of view, imaged in a label-free (e.g. brightfield) and labelled condition (e.g. fluorescent protein). When trained, this allows the user to identify certain structures from brightfield images alone. The performance of fnet may depend significantly on the structure at hand. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "3D",
        "ZeroCostDL4Mic",
        "labelling",
        "fnet"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_fnet_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4290839/files/jones_vstaining_tensorflow_saved_model_bundle.zip",
            "sha256": "ec37989b50017f53c3cb9cb3ff1a302ee3b7ed50ab6d8ba34264f10a64ad551a",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/jones_virtual_staining",
      "id": "JonesVirtualStaining",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/jones_virtual_staining/model.yaml",
      "download_url": "https://zenodo.org/record/4290839/files/jones_virtual_staining.bioimage.io.model.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Jones Virtual Staining",
      "description": "A trained GAN to transform wide-field autofluorescence images of unlabelled tissue sections into images that are equivalent to the bright-field images of histologically stained versions of the same samples.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Biomedical Engineering, 2019",
          "doi": "https://doi.org/10.1038/s41551-019-0362-y"
        }
      ],
      "authors": [
        "Yair Rivenson",
        "Hongda Wang",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "deepimagej",
        "Virtual staining",
        "GAN",
        "histology"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": null
    },
    {
      "type": "model",
      "id": "HPA-wienerschnitzelgemeinschaft",
      "name": "HPA-wienerschnitzelgemeinschaft",
      "tags": [
        "resnet-18",
        "hpa",
        "hill-climbing",
        "preresnet-50",
        "resnet-50",
        "airnext-50",
        "resnet-101",
        "inception-resnet-v2",
        "cbam",
        "inception-v3",
        "se-resnext-50",
        "airnet-50",
        "classification",
        "resnet-34"
      ],
      "authors": [
        "Shaikat Mahmood Galib",
        "Christof Henkel",
        "Kevin Hwang",
        "Dmytro Poplavskiy",
        "Bojan Tunguz",
        "Russ Wolfinger"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wienerschnitzelgemeinschaft",
      "description": "An ensemble of diverse and highly optimized CNN models, using hill climbing and weighted voting.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wienerschnitzelgemeinschaft/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "4th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-vpp",
      "name": "HPA-vpp",
      "tags": [
        "hpa",
        "inception-v4",
        "xception",
        "inception-v3",
        "multilayer-perceptron",
        "classification"
      ],
      "authors": [
        "Yinzheng Gu",
        "Chuanpeng Li",
        "Jinbin Xie"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/vpp",
      "description": "An ensemble of seven CNN models and a multi-layer perceptron network, using image augmentation, multi scales, weighted sampling and MultiLabelSoftMargin loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/vpp/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "5th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-pudae",
      "name": "HPA-pudae",
      "tags": [
        "hpa",
        "inception-v3",
        "se-resnext-50",
        "classification",
        "resnet-34"
      ],
      "authors": [
        "Park Jinmo"
      ],
      "license": "BSD-2-Clause",
      "git_repo": "https://github.com/CellProfiling/pudae-kaggle-hpa",
      "description": "An ensemble using focal loss, per image normalization and spatial attention.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/pudae-kaggle-hpa/master/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "3rd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-conv",
      "name": "HPA-conv is all you need",
      "tags": [
        "hpa",
        "xception",
        "inception-v3",
        "se-resnext-50",
        "classification"
      ],
      "authors": [
        "Xuan Cao",
        "Runmin Wei",
        "Yuanhao Wu",
        "Xun Zhu"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/conv_is_all_you_need",
      "description": "An ensemble of a cropping window CNN based on Xception, and two conventional CNNs based on SE-ResNext50 and InceptionV3.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/conv_is_all_you_need/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "10th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-bestfitting",
      "name": "HPA-bestfitting",
      "tags": [
        "hpa",
        "resnet-50",
        "inception-v3",
        "densenet-121",
        "classification",
        "resnet-34"
      ],
      "authors": [
        "Shubin Dai"
      ],
      "license": "MIT",
      "cite": null,
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting",
      "description": "A CNN model using focal loss and image augmentation, optimized with Adam optimizer.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "1st",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png",
        "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/src/bestfitting-densenet-diagram.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-WAIR",
      "name": "HPA-WAIR",
      "tags": [
        "hpa",
        "opencv",
        "xception",
        "scikit-learn",
        "se-resnext-50",
        "ibn-densenet-121",
        "densenet-169",
        "classification",
        "densenet-121"
      ],
      "authors": [
        "Jun Lan"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wair",
      "description": "Seven CNN models, using multiple different architectures, ensembled through averaging.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wair/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "2nd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-Random-Walk",
      "name": "HPA-Random Walk",
      "tags": [
        "attention-gate",
        "hpa",
        "classification",
        "resnet-18"
      ],
      "authors": [
        "Zhifeng Gao",
        "Cheng Ju",
        "Xiaohan Yi",
        "Hongdong Zheng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/random_walk",
      "description": "An ensemble model of three different resolutions based on single attention gated network.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/random_walk/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "38th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-One-More",
      "name": "HPA-One More Layer Of Stacking",
      "tags": [
        "hpa",
        "inception-v4",
        "bn-inception",
        "xception",
        "lightgbm",
        "se-resnext-50",
        "classification"
      ],
      "authors": [
        "Dmitry Buslov",
        "Sergei Fironov",
        "Alexander Kiselev",
        "Dmytro Panchenko"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/one_more_layer_of_stacking",
      "description": "14 CNN models ensembled via LightGBM stacking, optimized with Wadam, using focal and LSEP loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/one_more_layer_of_stacking/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "8th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-NTU_MiRA",
      "name": "HPA-NTU_MiRA",
      "tags": [
        "hpa",
        "classification",
        "resnet-34"
      ],
      "authors": [
        "Kuan-Lun Tseng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/ntu_mira",
      "description": "A CNN with large input size (1024 \u2a09 1024 px) using fixed batch-normalization and data distillation.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/ntu_mira/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "16th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/models/HPAShuffleNetV2",
      "id": "HPAShuffleNetV2",
      "source": "https://raw.githubusercontent.com/oeway/Anet-Model-Zoo/master/4_hpa_shufflenet_v2/model.json",
      "links": [
        "HPA-Classification"
      ],
      "name": "HPA ShuffleNetV2",
      "description": "A light-weight model for HPA image classification competition",
      "cite": null,
      "authors": [
        "Moshe Livne",
        "Wei OUYANG"
      ],
      "documentation": "HPAShuffleNetV2.md",
      "tags": [
        "hpa",
        "imjoy",
        "shufflenet",
        "classification",
        "tensorflow.js"
      ],
      "git_repo": "https://github.com/CellProfiling/HPA-Special-Prize",
      "badges": [
        {
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg",
          "label": "Powered by ImJoy"
        }
      ],
      "license": "Apache 2.0",
      "format_version": "0.1.0",
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4156050/files/model_v1.zip",
            "sha256": "5e3588d626a0f8ee655c503b4f867eacf53fe890feaa5c62125d4d2c544638f0",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          },
          "tensorflow_js": {
            "source": "https://raw.githubusercontent.com/deepimagej/tensorflow-js-models/main/fru-net_sev_segmentation_tf_js_model/model.json",
            "sha256": "eb27d8f3df23ce5d1d6ae4112cf36117c70a2257ca6f1b56869b9537fa60531e",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation",
      "id": "FRUNet2DsEVSegmentation",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation/model.yaml",
      "links": [
        "deepimagej"
      ],
      "download_url": "https://zenodo.org/record/4156050/files/deepimagej_fru-net_sev_segmentation.zip",
      "format_version": "0.3.0",
      "name": "Fully Residual U-Net - TEM",
      "description": "DeepImageJ compatible fully residual U-Net trained to segment small extracellular vesicles in 2D TEM images",
      "cite": [
        {
          "text": "G\u00f3mez-de-Mariscal, E. et al., Deep-Learning-Based Segmentation of SmallExtracellular Vesicles in Transmission Electron Microscopy Images Scientific Reports, (2019)",
          "doi": "https://doi.org/10.1038/s41598-019-49431-3"
        }
      ],
      "authors": [
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Martin Ma\u0161ka, Anna Kotrbov\u00e1",
        "Vendula Posp\u00edchalov\u00e1",
        "Pavel Matula",
        "Arrate Mu\u00f1oz-Barrutia"
      ],
      "documentation": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "covers": [
        "frunet_sev.jpg"
      ],
      "tags": [
        "TEM",
        "deepimagej",
        "segmentation",
        "extracellular vesicles"
      ],
      "license": "BSD 3"
    },
    {
      "type": "notebook",
      "id": "Notebook_DenoiSeg_2D_ZeroCostDL4Mic",
      "name": "DenoiSeg (2D) - ZeroCostDL4Mic",
      "description": "DenoiSeg 2D is deep-learning method that can be used to jointly denoise and segment 2D microscopy images. The benefits of using DenoiSeg (compared to other Deep Learning-based segmentation methods) are more prononced when only a few annotated images are available. However, the denoising part requires many images to perform well. All the noisy images don't need to be labeled to train DenoiSeg. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CycleGAN",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_Deep-STORM_ZeroCostDL4Mic",
      "name": "Deep-STORM training and example dataset - ZeroCostDL4Mic",
      "description": "Time-series of simulated, randomly distributed single-molecule localization (SMLM) data (Training dataset). Experimental time-series dSTORM acquisition of Glial cells stained with phalloidin for actin (Example dataset).",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Christophe Leterrier",
        "Romain F. Laine"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3959089",
      "tags": [
        "SMLM",
        "ZeroCostDL4Mic",
        "Deep-STORM",
        "2D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3959089",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4155785/files/deepstorm_zerocostdl4mic_tf_model.zip",
            "sha256": "dbacc73190690957a332281650f183b3d70db3cb17fe091ae39f84126fbac7fa",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          },
          "keras_hdf5": {
            "source": "https://raw.githubusercontent.com/deepimagej/models/master/deepstorm_zerocostdl4mic/weights_best.hdf5",
            "sha256": "61fd2b4d03d7704c8db810fbd33c6f6a4aed0b3a1ca8371c8381adb6850711f6",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/deepstorm_zerocostdl4mic",
      "id": "DeepSTORMZeroCostDL4Mic",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/deepstorm_zerocostdl4mic/model.yaml",
      "download_url": "https://zenodo.org/record/4155785/files/DeepImageJ_DeepSTORM_ZeroCostDL4Mic.zip",
      "links": [
        "zero/Notebook_Deep-STORM_2D_ZeroCostDL4Mic_DeepImageJ",
        "zero/Dataset_Deep-STORM_ZeroCostDL4Mic",
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "Deep-STORM - ZeroCostDL4Mic",
      "description": "A trained Deep-STORM model for image reconstruction from high-density single-molecule localization microscopy (SMLM).",
      "cite": [
        {
          "text": "Nehme E. et al., Optica 2018",
          "doi": "https://doi.org/10.1364/OPTICA.5.000458"
        },
        {
          "text": "Lucas von Chamier et al. biorXiv 2020",
          "doi": "https://doi.org/10.1101/2020.03.20.000133"
        },
        {
          "text": "G\u00f3mez de Mariscal et al. bioRxiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        }
      ],
      "authors": [
        "ZeroCostDL4Mic team",
        "DeepImageJ team"
      ],
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "documentation": "https://deepimagej.github.io/deepimagej",
      "covers": [
        "input.png",
        "zoom.png"
      ],
      "tags": [
        "SMLM",
        "super-resolution",
        "image reconstruction",
        "zerocostdl4mic",
        "deepimagej"
      ],
      "license": "MIT"
    },
    {
      "type": "notebook",
      "id": "Notebook_Deep-STORM_2D_ZeroCostDL4Mic_DeepImageJ",
      "name": "Deep-STORM (2D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by Nehme et al. in Optica. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via DeepImageJ.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the DeepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/Deep-STORM_2D_ZeroCostDL4Mic_Fiji_export.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "DeepImageJ",
        "Deep-STORM",
        "2D"
      ],
      "source": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/Deep-STORM_2D_ZeroCostDL4Mic_Fiji_export.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Deep-STORM_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_Deep-STORM_2D_ZeroCostDL4Mic",
      "name": "Deep-STORM (2D) - ZeroCostDL4Mic",
      "description": "Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by Nehme et al. in Optica. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "Deep-STORM",
        "labelling",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Deep-STORM_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4269855/files/defcon_density_map_estimation_tf_model.zip",
            "sha256": "ea57590caefa41a493808420d5bc029d7b6c8ad8b1633d8feeb166a99d71f45d",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          },
          "tensorlfow_js": {
            "source": "https://raw.githubusercontent.com/deepimagej/tensorflow-js-models/main/defcon_density_map_estimation_tf_js_model/model.json",
            "sha256": "d1427f81f1fb217c127ef01ed992b6a9dc11d3a5b552514cfdccd82c01230e21",
            "test_input": [
              "./exampleImage.tiff"
            ],
            "test_output": [
              "./resultImage.tiff"
            ]
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/defcon_density_map_estimation",
      "id": "DEFCoN_DensityMapEstimation",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/defcon_density_map_estimation/model.yaml",
      "download_url": "https://zenodo.org/record/4269855/files/deepimagej_defcon_density_map_estimation.zip",
      "links": [
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "DEFCoN density map estimation",
      "description": "Density Estimation by Fully Convolutional Networks (DEFCoN) - A fluorescent spot counter for single molecule localization microscopy.",
      "cite": [
        {
          "text": "DEFCoN was written by Baptiste Ottino as a Master's thesis project under the guidance of Kyle M. Douglass and Suliana Manley in the Laboratory of Experimental Biophysics."
        }
      ],
      "authors": [
        "Baptiste Ottino",
        "Kyle M. Douglass",
        "Suliana Manley"
      ],
      "documentation": "https://github.com/LEB-EPFL/DEFCoN-ImageJ/wiki",
      "covers": [
        "cover_image.jpg"
      ],
      "tags": [
        "smlm",
        "deepimagej",
        "defcon",
        "density estimation"
      ],
      "license": "BSD 3",
      "git_repo": "https://github.com/LEB-EPFL/DEFCoN"
    },
    {
      "type": "dataset",
      "id": "Dataset_CycleGAN_ZeroCostDL4Mic",
      "name": "CycleGAN example training and test dataset - ZeroCostDL4Mic",
      "description": "Unpaired microscopy images (fluorescence) of microtubules (Spinning-disk and SRRF reconstructed images)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "CycleGAN"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941884",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/unpaired-image_translation.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_CycleGAN_2D_ZeroCostDL4Mic",
      "name": "CycleGAN (2D) - ZeroCostDL4Mic",
      "description": "CycleGAN is a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples (ie transform a horse into zebra or apples into oranges). While CycleGAN can potentially be used for any type of image-to-image translation, we illustrate that it can be used to predict what a fluorescent label would look like when imaged using another imaging modalities. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CycleGAN",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CycleGAN_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_StarDist_brightfield2_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 3 - Flow chamber dataset",
      "description": "Paired brightfield images of cancer cells and corresponding masks",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Gautier Follain",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "source": "https://zenodo.org/record/4034939",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingflo_trackmate.png"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_StarDist_brightfield_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 2 - T cell dataset",
      "description": "Paired brightfield images of migrating T cells and corresponding masks",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Nathan H. Roy",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "source": "https://zenodo.org/record/4034929",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingTcells_trackmate.png"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_StarDist_Fluo_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 1 - Breast cancer cell dataset",
      "description": "Fluorescence microscopy of Nuclei (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "source": "https://zenodo.org/record/4034976",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingfluo_trackmate.png"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713337",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713337",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "id": "Dataset_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713330",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "2D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713330",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "CARE",
        "denoising",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "id": "Notebook_Augmentor_ZeroCostDL4Mic",
      "name": "Augmentor - ZeroCostDL4Mic",
      "description": "Augmentor is a data augmentation library. Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation can be especially valuable when training dataset need to be manually labelled. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "Data Augmentation",
        "Augmentor"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/platybrowser/platybrowser/3711f1c26e5db8c38c3faff4cccb3110560e3c67/segmentation/cells/UNet3DPlatyCellProbs.model",
      "id": "UNet3DPlatyCellProbs",
      "source": "mmpb.segmentation.network.models.UNetAnisotropic",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/platybrowser/platybrowser/releases/download/1.0.0/UNet3DPlatyCellProbs.model.zip",
      "name": "3D UNet Platynereis Cell Segmentation (Probabilities)",
      "description": "A 3d U-Net trained to predict the cell boundaries in a EM volume of a 6 day old Platynereis.",
      "cite": [
        {
          "text": "Vergara, Hernando M. et al. Whole-body integration of gene expression and single-cell morphology. BioRxiv 2020.\"",
          "doi": "https://doi.org/10.1101/2020.02.26.961037"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "cell membrane",
        "EM",
        "platynereis",
        "ilastik",
        "unet3d",
        "pytorch",
        "segmentation"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_raw.png",
        "ilastik_pred.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/wolny/pytorch-3dunet/37f186c80f4d64b1dab5d165d8c2aae15b5aede1/bioimage-io/UNet3DArabidopsisOvules.model",
      "id": "UNet3DArabidopsisOvules",
      "source": "pytorch3dunet.unet3d.model.UNet3D",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/wolny/pytorch-3dunet/releases/download/1.2.6/UNet3DArabidopsisOvules.model.zip",
      "name": "3D UNet Arabidopsis Ovules",
      "description": "A 3d U-Net trained to predict the cell boundaries in confocal stacks of Arabidopsis ovules. Voxel size: (0.235, 0.150, 0.150) microns ZYX",
      "cite": [
        {
          "text": "Wolny, Adrian et al. Accurate and Versatile 3D Segmentation of Plant Tissues at Cellular Resolution. BioRxiv 2020.",
          "doi": "https://doi.org/10.1101/2020.01.17.910562"
        }
      ],
      "authors": [
        "Adrian Wolny;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "cell membrane",
        "plant tissue",
        "ilastik",
        "unet3d",
        "ovuls",
        "arabidopsis",
        "pytorch",
        "segmentation"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_4.png",
        "ilastik_5.png",
        "ilastik_6.png",
        "ilastik_7.png",
        "ilastik_8.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/pytorch-bioimage-io/v0.1.1/specs/models/unet2d/nuclei_broad",
      "id": "UNet2DNucleiBroad",
      "source": "pybio.torch.models.unet.UNet2d",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/bioimage-io/pytorch-bioimage-io/releases/download/v0.1.1/UNet2DNucleiBroad.model.zip",
      "name": "2D UNet Nuclei Broad",
      "description": "A 2d U-Net pretrained on broad nucleus dataset.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io",
        "Fynn Beuttenm\u00fcller"
      ],
      "documentation": "UNet2DNucleiBroad.md",
      "tags": [
        "pytorch",
        "unet2d",
        "ilastik",
        "nucleus-segmentation"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "cover0.png"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://zenodo.org/record/4155785/files/2du-net_zerocostdl4mic_tf_model.zip",
            "sha256": "167a552f5b97b5bf5e3da2c44755a5c6f5e4d2b019ef9151627495e9f91434ef",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          },
          "keras_hdf5": {
            "source": "https://raw.githubusercontent.com/deepimagej/models/master/2du-net_zerocostdl4mic/weights_best.hdf5",
            "sha256": "52f4065fe49f3e3bbd21bf36c815f6431042a1d6699b3caea40572cae833c893",
            "test_inputs": "./exampleImage.tiff",
            "test_outputs": "./resultImage.tiff"
          }
        }
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/2du-net_zerocostdl4mic",
      "id": "2DUNetZeroCostDL4Mic",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/2du-net_zerocostdl4mic/model.yaml",
      "download_url": "https://zenodo.org/record/4155785/files/DeepImageJ_2D%20UNet_ZeroCostDL4Mic.zip",
      "links": [
        "zero/Notebook_U-Net_2D_ZeroCostDL4Mic_DeepImageJ",
        "deepimagej"
      ],
      "format_version": "0.3.0",
      "name": "2D UNet - ZeroCostDL4Mic",
      "description": "2D U-Net trained for binary segmentation using the EM images of neuronal membranes and segmentation masks from the ISBI segmentation challenge 2012.",
      "cite": [
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        },
        {
          "text": "Lucas von Chamier et al. bioRxiv 2020",
          "doi": "https://doi.org/10.1101/2020.03.20.000133"
        },
        {
          "text": "G\u00f3mez de Mariscal et al. bioRxiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        }
      ],
      "authors": [
        "ZeroCostDL4Mic team",
        "DeepImageJ team"
      ],
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "documentation": "https://deepimagej.github.io/deepimagej",
      "covers": [
        "cover.png"
      ],
      "tags": [
        "zerocostdl4mic",
        "deepimagej",
        "unet",
        "segmentation"
      ],
      "license": "MIT"
    },
    {
      "type": "notebook",
      "id": "unet-pancreaticcellsegmentation",
      "name": "2D U-Net for binary segmentation",
      "description": "Easy example to define a 2D U-Net for segmentation with Keras and import it into DeepImageJ format",
      "cite": [
        {
          "text": "Falk, T., Mai, D., Bensch, R. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods 16, 67\u201370 (2019).",
          "doi": "https://doi.org/10.1038/s41592-018-0261-2"
        }
      ],
      "authors": [
        "Ignacio Arganda-Carreras",
        "DeepImageJ team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/notebook_intro.png",
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/usecase.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/deepimagej/models/blob/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
        }
      ],
      "documentation": "https://github.com/miura/NEUBIAS_AnalystSchool2020/tree/master/Ignacio",
      "tags": [
        "deepimagej",
        "unet",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb",
      "links": [
        "UNet2DPancreaticSegmentation"
      ]
    }
  ]
}