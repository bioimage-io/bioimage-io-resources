{
  "id": "bioimage.io",
  "name": "BioImage.IO",
  "tags": [],
  "logo": "\ud83e\udd92",
  "icon": "\ud83e\udd92",
  "splash_title": "Bioimage Model Zoo",
  "splash_subtitle": "Advanced AI models in one-click",
  "splash_feature_list": [
    "Integrate with Fiji, Ilastik, ImJoy",
    "Try model instantly with BioEngine",
    "Contribute your models via Github",
    "Link models to datasets and applications"
  ],
  "explore_button_text": "Start Exploring",
  "background_image": "static/img/zoo-background.svg",
  "resource_types": [
    "model",
    "application",
    "notebook",
    "dataset"
  ],
  "default_type": "model",
  "url_root": "https://raw.githubusercontent.com/bioimage-io/bioimage-io-models/master",
  "collections": [
    {
      "id": "ilastik",
      "name": "ilastik",
      "tags": [
        "ilastik"
      ],
      "logo": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "splash_title": "ilastik",
      "splash_subtitle": "the interactive learning and segmentation toolkit",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model",
      "url_root": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master"
    },
    {
      "id": "deepimagej",
      "name": "deepImageJ",
      "tags": [
        "deepimagej"
      ],
      "logo": "https://raw.githubusercontent.com/deepimagej/models/master/logos/logo.png",
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "splash_title": "deepImageJ",
      "splash_subtitle": "A user-friendly plugin to run deep learning models in ImageJ",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/deepimagej/models/master"
    },
    {
      "id": "zero",
      "name": "ZeroCostDL4Mic",
      "version": "1.7.1",
      "tags": [
        "ZeroCostDL4Mic"
      ],
      "logo": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "icon": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "splash_title": "ZeroCostDL4Mic",
      "splash_subtitle": "A Google Colab based no-cost toolbox to explore Deep-Learning in Microscopy",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook",
        "dataset"
      ],
      "default_type": "notebook",
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "imjoy",
      "name": "ImJoy",
      "tags": [
        "imjoy"
      ],
      "logo": "https://imjoy.io/static/img/imjoy-icon.svg",
      "icon": "https://imjoy.io/static/img/imjoy-icon.svg",
      "splash_title": "ImJoy",
      "splash_subtitle": "Deep Learning Made Easy!",
      "splash_feature_list": [
        "Minimal and flexible plugin powered web application",
        "Server-less progressive web application with offline support",
        "Rich and interactive user interface powered by web technologies"
      ],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "notebook",
        "application"
      ],
      "default_type": "application",
      "url_root": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master"
    },
    {
      "id": "hpa",
      "name": "HPA",
      "tags": [
        "hpa"
      ],
      "logo": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "about_url": "https://www.proteinatlas.org/",
      "splash_title": "The Human Protein Atlas",
      "splash_subtitle": null,
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model"
    },
    {
      "id": "fiji",
      "name": "Fiji",
      "tags": [
        "fiji"
      ],
      "logo": "https://fiji.sc/site/logo.png",
      "icon": "https://fiji.sc/site/logo.png",
      "splash_title": "Fiji",
      "splash_subtitle": "Fiji is just ImageJ",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    }
  ],
  "resources": [
    {
      "id": "Notebook Preview",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/nbpreview/master/notebook-preview.imjoy.html",
      "name": "Notebook Preview",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "Previewing Jupyter notebook without a Jupyter server",
      "tags": [
        "notebook",
        "jupyter",
        "imjoy"
      ],
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "Kaibu",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/kaibu/master/Kaibu.imjoy.html",
      "name": "Kaibu",
      "version": "0.1.12",
      "api_version": "0.2.3",
      "description": "Kaibu--a web application for visualizing and annotating multi-dimensional images",
      "tags": [],
      "covers": [
        "https://raw.githubusercontent.com/imjoy-team/kaibu/master/public/static/img/kaibu-screenshot-1.png"
      ],
      "badges": [
        {
          "icon": "https://imjoy.io/static/badge/launch-imjoy-badge.svg",
          "label": "Launch ImJoy",
          "url": "https://imjoy.io/#/app?plugin=https://kaibu.org/#/app"
        },
        {
          "icon": "https://mybinder.org/badge_logo.svg",
          "label": "Launch Binder",
          "url": "https://mybinder.org/v2/gist/oeway/690c2e62311223ae93e644d542eb8949/master?filepath=Kaibu-jupyter-tutorial.ipynb"
        }
      ],
      "authors": [
        "ImJoy-Team"
      ]
    },
    {
      "id": "Ilastik",
      "type": "application",
      "source": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/src/Ilastik-app.imjoy.html",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "name": "Ilastik",
      "version": "0.1.0",
      "api_version": "0.1.7",
      "description": "Ilastik Model Preview for BioImage.io",
      "requirements": [
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"
      ],
      "dependencies": [
        "https://gist.githubusercontent.com/oeway/2d4b5899424a14d8e90ad908d4cec364/raw/TiktorchModelLoader.imjoy.html",
        "https://gist.githubusercontent.com/oeway/f09955746ec01a20053793aba83c3545/raw/CompareImages.imjoy.html"
      ],
      "env": "",
      "tags": [],
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "HPA-Classification",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/HPA-Classification.imjoy.html",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "name": "HPA-Classification",
      "version": "0.2.1",
      "api_version": "0.1.7",
      "description": "ShuffleNetV2 for HPA.",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/3.13.1/js-yaml.min.js",
        "https://cdn.jsdelivr.net/npm/apexcharts",
        "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs",
        "https://cdn.jsdelivr.net/npm/simpleheat@0.4.0/simpleheat.min.js",
        "https://cdn.jsdelivr.net/gh/photopea/UTIF.js@4f1b10cb09e244cfd4f9631245d2231537148be7/UTIF.js"
      ],
      "dependencies": [
        "https://raw.githubusercontent.com/imjoy-team/example-plugins/master/imjoy-plugins/HPA-Image-Selection.imjoy.html"
      ],
      "env": null,
      "tags": [],
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ],
      "badges": [],
      "authors": []
    },
    {
      "id": "Fiji",
      "name": "Fiji",
      "description": "Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis.",
      "source": "https://fiji.sc/",
      "cite": {
        "text": "Schindelin, J., Arganda-Carreras, I., Frise, E. et al. Fiji: an open-source platform for biological-image analysis. Nat Methods 9, 676\u2013682 (2012).",
        "doi": "https://doi.org/10.1038/nmeth.2019"
      },
      "authors": [
        "Fiji community"
      ],
      "icon": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/Fiji-icon.png",
      "documentation": "https://fiji.sc/",
      "git_repo": "https://github.com/fiji/fiji",
      "tags": [
        "fiji"
      ],
      "type": "application"
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_pix2pix_ZeroCostDL4Mic",
      "name": "pix2pix example training and test dataset - ZeroCostDL4Mic",
      "description": "Paired microscopy images (fluorescence) of lifeact-RFP and sir-DNA",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941889",
      "tags": [
        "ZeroCostDL4Mic",
        "pix2pix"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941889",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/paired-image_translation.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_pix2pix_2D_ZeroCostDL4Mic",
      "name": "pix2pix (2D) - ZeroCostDL4Mic",
      "description": "pix2pix is a deep-learning method that can be used to translate one type of images into another. While pix2pix can potentially be used for any type of image-to-image translation, we demonstrate that it can be used to predict a fluorescent image from another fluorescent image. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "2D",
        "pix2pix"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_pix2pix_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_YOLOv2_ZeroCostDL4Mic",
      "name": "YoloV2 example training and test dataset - ZeroCostDL4Mic",
      "description": "2D grayscale .png images with corresponding bounding box annotations in .xml  PASCAL Voc format.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet",
        "Lucas von Chamier"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941908",
      "tags": [
        "ZeroCostDL4Mic",
        "YOLOv2"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941908",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://gist.githubusercontent.com/oeway/582856630a0aed4d4d221e54df1b3ece/raw",
      "id": "vitessce-image-viewer",
      "source": "https://gist.githubusercontent.com/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/raw/vitessce-image-viewer-imjoy-demo.ipynb",
      "links": [
        "Notebook Preview"
      ],
      "name": "Vitessce Image Viewer",
      "description": "Use vitessce-image-viewer in Jupyter notebooks with ImJoy Jupyter Extension",
      "cite": null,
      "authors": [
        "Wei OUYANG"
      ],
      "badges": [
        {
          "label": "Launch Binder",
          "icon": "https://mybinder.org/badge_logo.svg",
          "url": "https://mybinder.org/v2/gist/oeway/ebedc17c9ab1f6aa5eee181679d85b5f/master?filepath=vitessce-image-viewer-imjoy-demo.ipynb"
        },
        {
          "label": "Powered by ImJoy",
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg"
        }
      ],
      "tags": [
        "imjoy",
        "visualization"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "UNet2DPancreaticSegmentation",
      "source": "https://github.com/deepimagej/models/u-net_pancreatic_segmentation/U_Net_PhC-C2DL-PSC_segmentation.ipynb",
      "links": [
        "unet-pancreaticcellsegmentation"
      ],
      "name": "U-Net Pancreatic Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment phase contrast microscopy images of pancreatic stem cells on a 2D polystyrene substrate.",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal E. et al., biorXiv 2019; Ulman V. et al., Nature Methods 2017; Ronneberger O. et al., MICCAI 2015",
        "doi": null
      },
      "authors": [
        "DeepImageJ",
        "Ignacio Arganda-Carreras"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "segmentation",
        "phase contrast",
        "deepimagej",
        "pancreatic stem cells"
      ],
      "license": null,
      "format_version": null,
      "model": {
        "source": "./saved_model.pb",
        "sha256": null,
        "v1": {
          "source": "./variables",
          "sha256": null
        }
      }
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation",
      "id": "UNet2DHelaSegmentation",
      "source": "https://github.com/deepimagej/python4deepimagej/tree/master/unet",
      "name": "U-Net Hela Cell Segmentation",
      "description": "DeepImageJ compatible U-Net trained to segment Hela cells in 2D phase contrast microscopy images",
      "cite": {
        "text": "Biomedical Imaging Group, School of Engineering, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland",
        "doi": null
      },
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": null,
      "tags": [
        "hela cells",
        "phase contrast",
        "deepimagej",
        "segmentation"
      ],
      "license": null,
      "format_version": null,
      "model": {
        "source": "./saved_model.pb",
        "sha256": "601830ca4462cc7b6d1047541bd2e6de105dbc92d6da42d840700eaf65aef0e7",
        "v1": {
          "source": "./variables",
          "sha256": null
        }
      }
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "UNetDA",
      "source": "src.utils.get_unet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/UNetDA.model.zip",
      "name": "U-Net DA (Domain Adaptation)",
      "description": "U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "sga2",
        "hbp",
        "brain",
        "unet2d",
        "pytorch",
        "ilastik"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/UNetCover.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_U-Net_3D_ZeroCostDL4Mic",
      "name": "U-Net (3D) - ZeroCostDL4Mic",
      "description": "The 3D U-Net was first introduced by \u00c7i\u00e7ek et al for learning dense volumetric segmentations from sparsely annotated ground-truth data building upon the original U-Net architecture by Ronneberger et al. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Daniel Krentzel and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "U-Net",
        "ZeroCostDL4Mic",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_U-Net_2D_ZeroCostDL4Mic",
      "name": "U-Net (2D) - ZeroCostDL4Mic",
      "description": "U-Net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-Net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "U-Net",
        "ZeroCostDL4Mic",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "2sUNetDA",
      "source": "src.utils.get_2sunet",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/2sUNetDA.model.zip",
      "name": "Two Steam U-Net DA",
      "description": "Two Steam U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). Two Steam U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. Two Steam U-Net uses differet encoders to process inputs from source and target, and use a common decoder to generate the respective segmentation outputs. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Roger Bermudez et al. A domain-adaptive two-stream U-Net for electron microscopy image segmentation. ISBI 2018.",
          "doi": "https://doi.org/10.1109/ISBI.2018.8363602"
        }
      ],
      "authors": [
        "Roger Bermudez, Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "vasculature",
        "sga2",
        "hbp",
        "brain",
        "unet2d",
        "pytorch",
        "ilastik"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/2sUNetCover.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_StarDist_3D_ZeroCostDL4Mic",
      "name": "StarDist (3D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 3D (xyz) images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "3D",
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
      "name": "StarDist (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Johanna Jukkala",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3715492",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "StarDist",
        "2D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3715492",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Stardist_nuclei_masks.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_StarDist_2D_ZeroCostDL4Mic",
      "name": "StarDist (2D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 2D (xy) single images or in stacks (xyz). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "segmentation",
        "ZeroCostDL4Mic",
        "StarDist",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_StarDist_2D_ZeroCostDL4Mic_2D"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2Void (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713326",
      "tags": [
        "Noise2Void",
        "denoising",
        "ZeroCostDL4Mic",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713326",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (paxillin-GFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Aki Stubb",
        "Guillaume Jacquemet",
        "Johanna Ivaska"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713315",
      "tags": [
        "Noise2Void",
        "denoising",
        "ZeroCostDL4Mic",
        "2D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713315",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/N2V_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) - ZeroCostDL4Mic",
      "description": "Noise2Void 2D is deep-learning method that can be used to denoise 2D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "Noise2VOID",
        "denoising",
        "ZeroCostDL4Mic",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2Void_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2VOID (3D) - ZeroCostDL4Mic",
      "description": "Noise2VOID 3D is deep-learning method that can be used to denoise 3D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "Noise2Void",
        "denoising",
        "ZeroCostDL4Mic",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Noise2Void_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/v0.1.4/models/n2v-sem-demo",
      "id": "N2VSEMDemo",
      "source": "de.csbdresden.n2v.train.N2VPrediction",
      "links": [
        "Fiji"
      ],
      "download_url": "https://github.com/bioimage-io/fiji-bioimage-io/releases/download/v0.1.4/n2v-sem-demo.zip",
      "name": "N2V SEM Demo",
      "description": "Demo model for denoising trained on a single SEM image with Noise2Void",
      "cite": {
        "text": "Buchholz, T. et al. - Content-aware image restoration for electron microscopy. \nMethods in Cell Biology, Volume 152 p.277-289, ISSN 0091-679X (2019)",
        "doi": "https://doi.org/10.1016/bs.mch.2019.05.001"
      },
      "authors": [
        "Deborah Schmidt"
      ],
      "documentation": "README.md",
      "covers": [
        "thumbnail.png"
      ],
      "tags": [
        "fiji",
        "denoising",
        "unet2d",
        "n2v"
      ],
      "license": "BSD 3",
      "format_version": "0.1.0"
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_fnet_3D_ZeroCostDL4Mic",
      "name": "Label-free prediction (fnet) example training and test dataset - ZeroCostDL4Mic",
      "description": "Confocal microscopy data (TOM20 labeled with Alexa Fluor 594)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Christoph Spahn"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3748967",
      "tags": [
        "fnet",
        "ZeroCostDL4Mic",
        "labelling",
        "3D"
      ],
      "source": "https://doi.org/10.5281/zenodo.3748967",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Fnet_exemplary_data_mitochondria.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_fnet_ZeroCostDL4Mic",
      "name": "Label-free Prediction - fnet - (3D) ZeroCostDL4Mic",
      "description": "Label-free Prediction (fnet) is a neural network used to infer the features of cellular structures from brightfield or EM images without coloured labels. The network is trained using paired training images from the same field of view, imaged in a label-free (e.g. brightfield) and labelled condition (e.g. fluorescent protein). When trained, this allows the user to identify certain structures from brightfield images alone. The performance of fnet may depend significantly on the structure at hand. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "fnet",
        "ZeroCostDL4Mic",
        "labelling",
        "3D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/fnet_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_fnet_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "model",
      "id": "HPA-wienerschnitzelgemeinschaft",
      "name": "HPA-wienerschnitzelgemeinschaft",
      "tags": [
        "preresnet-50",
        "resnet-101",
        "cbam",
        "airnet-50",
        "inception-resnet-v2",
        "inception-v3",
        "classification",
        "hpa",
        "airnext-50",
        "resnet-18",
        "se-resnext-50",
        "hill-climbing",
        "resnet-50",
        "resnet-34"
      ],
      "authors": [
        "Shaikat Mahmood Galib",
        "Christof Henkel",
        "Kevin Hwang",
        "Dmytro Poplavskiy",
        "Bojan Tunguz",
        "Russ Wolfinger"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wienerschnitzelgemeinschaft",
      "description": "An ensemble of diverse and highly optimized CNN models, using hill climbing and weighted voting.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wienerschnitzelgemeinschaft/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "4th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-vpp",
      "name": "HPA-vpp",
      "tags": [
        "inception-v4",
        "xception",
        "inception-v3",
        "classification",
        "multilayer-perceptron",
        "hpa"
      ],
      "authors": [
        "Yinzheng Gu",
        "Chuanpeng Li",
        "Jinbin Xie"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/vpp",
      "description": "An ensemble of seven CNN models and a multi-layer perceptron network, using image augmentation, multi scales, weighted sampling and MultiLabelSoftMargin loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/vpp/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "5th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-pudae",
      "name": "HPA-pudae",
      "tags": [
        "inception-v3",
        "classification",
        "hpa",
        "se-resnext-50",
        "resnet-34"
      ],
      "authors": [
        "Park Jinmo"
      ],
      "license": "BSD-2-Clause",
      "git_repo": "https://github.com/CellProfiling/pudae-kaggle-hpa",
      "description": "An ensemble using focal loss, per image normalization and spatial attention.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/pudae-kaggle-hpa/master/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "3rd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-conv",
      "name": "HPA-conv is all you need",
      "tags": [
        "xception",
        "inception-v3",
        "classification",
        "hpa",
        "se-resnext-50"
      ],
      "authors": [
        "Xuan Cao",
        "Runmin Wei",
        "Yuanhao Wu",
        "Xun Zhu"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/conv_is_all_you_need",
      "description": "An ensemble of a cropping window CNN based on Xception, and two conventional CNNs based on SE-ResNext50 and InceptionV3.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/conv_is_all_you_need/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "10th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-bestfitting",
      "name": "HPA-bestfitting",
      "tags": [
        "inception-v3",
        "classification",
        "hpa",
        "densenet-121",
        "resnet-50",
        "resnet-34"
      ],
      "authors": [
        "Shubin Dai"
      ],
      "license": "MIT",
      "cite": null,
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting",
      "description": "A CNN model using focal loss and image augmentation, optimized with Adam optimizer.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "1st",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png",
        "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/src/bestfitting-densenet-diagram.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-WAIR",
      "name": "HPA-WAIR",
      "tags": [
        "opencv",
        "ibn-densenet-121",
        "xception",
        "scikit-learn",
        "classification",
        "hpa",
        "se-resnext-50",
        "densenet-121",
        "densenet-169"
      ],
      "authors": [
        "Jun Lan"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wair",
      "description": "Seven CNN models, using multiple different architectures, ensembled through averaging.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wair/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "2nd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-Random-Walk",
      "name": "HPA-Random Walk",
      "tags": [
        "classification",
        "hpa",
        "attention-gate",
        "resnet-18"
      ],
      "authors": [
        "Zhifeng Gao",
        "Cheng Ju",
        "Xiaohan Yi",
        "Hongdong Zheng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/random_walk",
      "description": "An ensemble model of three different resolutions based on single attention gated network.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/random_walk/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "38th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-One-More",
      "name": "HPA-One More Layer Of Stacking",
      "tags": [
        "lightgbm",
        "inception-v4",
        "xception",
        "classification",
        "hpa",
        "se-resnext-50",
        "bn-inception"
      ],
      "authors": [
        "Dmitry Buslov",
        "Sergei Fironov",
        "Alexander Kiselev",
        "Dmytro Panchenko"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/one_more_layer_of_stacking",
      "description": "14 CNN models ensembled via LightGBM stacking, optimized with Wadam, using focal and LSEP loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/one_more_layer_of_stacking/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "8th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "HPA-NTU_MiRA",
      "name": "HPA-NTU_MiRA",
      "tags": [
        "classification",
        "hpa",
        "resnet-34"
      ],
      "authors": [
        "Kuan-Lun Tseng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/ntu_mira",
      "description": "A CNN with large input size (1024 \u2a09 1024 px) using fixed batch-normalization and data distillation.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/ntu_mira/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "16th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/models/HPAShuffleNetV2",
      "id": "HPAShuffleNetV2",
      "source": "https://raw.githubusercontent.com/oeway/Anet-Model-Zoo/master/4_hpa_shufflenet_v2/model.json",
      "links": [
        "HPA-Classification"
      ],
      "name": "HPA ShuffleNetV2",
      "description": "A light-weight model for HPA image classification competition",
      "cite": null,
      "authors": [
        "Moshe Livne",
        "Wei OUYANG"
      ],
      "documentation": "HPAShuffleNetV2.md",
      "tags": [
        "imjoy",
        "shufflenet",
        "classification",
        "hpa",
        "tensorflow.js"
      ],
      "git_repo": "https://github.com/CellProfiling/HPA-Special-Prize",
      "badges": [
        {
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg",
          "label": "Powered by ImJoy"
        }
      ],
      "license": "Apache 2.0",
      "format_version": "0.1.0",
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ]
    },
    {
      "type": "model",
      "attachments": {
        "weights": [
          {
            "id": "v1",
            "name": "version 1",
            "description": "weights trained for segmenting small extracellular vesicles",
            "source": "./variables",
            "sha256": null
          }
        ]
      },
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation",
      "id": "FRUNet2DsEVSegmentation",
      "source": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "download_url": "https://github.com/deepimagej/models/releases/download/0.1/fru-net_sev_segmentation.zip",
      "name": "Fully Residual U-Net - TEM",
      "description": "DeepImageJ compatible fully residual U-Net trained to segment small extracellular vesicles in 2D TEM images",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal, E. et al., Deep-Learning-Based Segmentation of SmallExtracellular Vesicles in Transmission Electron Microscopy Images \nScientific Reports, (2019)",
        "doi": "https://doi.org/10.1038/s41598-019-49431-3"
      },
      "authors": [
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Martin Ma\u0161ka",
        "Anna Kotrbov\u00e1",
        "Vendula Posp\u00edchalov\u00e1",
        "Pavel Matula and Arrate Mu\u00f1oz-Barrutia"
      ],
      "documentation": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "covers": [
        "frunet_sev.jpg"
      ],
      "tags": [
        "extracellular vesicles",
        "TEM",
        "deepimagej",
        "segmentation"
      ],
      "license": "BSD 3",
      "format_version": "0.2.0",
      "model": {
        "source": "./saved_model.pb",
        "sha256": "9ccb79070f30813e7447342e3ab7f4107a314a1414c1541c79134ef950ac4a7f"
      }
    },
    {
      "type": "dataset",
      "attachments": {
        "files": [
          "https://gist.githubusercontent.com/manzt/d16dbac0ea3adc3c7b9b61f54fa1f78d/raw/95854058512862accb0182d4a02f86a55ad19139/"
        ]
      },
      "root_url": "https://gist.githubusercontent.com/oeway/3dcbd79cd29da42be13a7e3bc0f9ca12/raw",
      "id": "dummy_zarr",
      "source": "https://gist.githubusercontent.com/oeway/3dcbd79cd29da42be13a7e3bc0f9ca12/raw/dummy_zarr.dataset.yaml",
      "name": "Dummy Zarr",
      "description": "A dummy zarr dataset",
      "cite": null,
      "authors": [
        "NO ONE"
      ],
      "documentation": null,
      "tags": [
        "zarr"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks",
      "id": "Notebook_DenoiSeg_2D_ZeroCostDL4Mic",
      "name": "DenoiSeg (2D) - ZeroCostDL4Mic",
      "description": "DenoiSeg 2D is deep-learning method that can be used to jointly denoise and segment 2D microscopy images. The benefits of using DenoiSeg (compared to other Deep Learning-based segmentation methods) are more prononced when only a few annotated images are available. However, the denoising part requires many images to perform well. All the noisy images don't need to be labeled to train DenoiSeg. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "CycleGAN",
        "ZeroCostDL4Mic",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_Deep-STORM_ZeroCostDL4Mic",
      "name": "Deep-STORM training and example dataset - ZeroCostDL4Mic",
      "description": "Time-series of simulated, randomly distributed single-molecule localization (SMLM) data (Training dataset). Experimental time-series dSTORM acquisition of Glial cells stained with phalloidin for actin (Example dataset).",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Christophe Leterrier",
        "Romain F. Laine"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3959089",
      "tags": [
        "SMLM",
        "ZeroCostDL4Mic",
        "2D",
        "Deep-STORM"
      ],
      "source": "https://doi.org/10.5281/zenodo.3959089",
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_Deep-STORM_2D_ZeroCostDL4Mic",
      "name": "Deep-STORM (2D) - ZeroCostDL4Mic",
      "description": "Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by Nehme et al. in Optica. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "ZeroCostDL4Mic",
        "labelling",
        "2D",
        "Deep-STORM"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_Deep-STORM_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_CycleGAN_ZeroCostDL4Mic",
      "name": "CycleGAN example training and test dataset - ZeroCostDL4Mic",
      "description": "Unpaired microscopy images (fluorescence) of microtubules (Spinning-disk and SRRF reconstructed images)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "CycleGAN",
        "ZeroCostDL4Mic"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941884",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/unpaired-image_translation.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_CycleGAN_2D_ZeroCostDL4Mic",
      "name": "CycleGAN (2D) - ZeroCostDL4Mic",
      "description": "CycleGAN is a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples (ie transform a horse into zebra or apples into oranges). While CycleGAN can potentially be used for any type of image-to-image translation, we illustrate that it can be used to predict what a fluorescent label would look like when imaged using another imaging modalities. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "CycleGAN",
        "ZeroCostDL4Mic",
        "2D"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CycleGAN_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713337",
      "tags": [
        "3D",
        "denoising",
        "ZeroCostDL4Mic",
        "CARE"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713337",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "3D",
        "denoising",
        "ZeroCostDL4Mic",
        "CARE"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_3D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "dataset",
      "root_url": "https://doi.org/10.5281",
      "id": "Dataset_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713330",
      "tags": [
        "denoising",
        "ZeroCostDL4Mic",
        "2D",
        "CARE"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713330",
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks",
      "id": "Notebook_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "denoising",
        "ZeroCostDL4Mic",
        "2D",
        "CARE"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview",
        "Dataset_CARE_2D_ZeroCostDL4Mic"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Tools",
      "id": "Notebook_Augmentor_ZeroCostDL4Mic",
      "name": "Augmentor - ZeroCostDL4Mic",
      "description": "Augmentor is a data augmentation library. Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation can be especially valuable when training dataset need to be manually labelled. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "Lucas von Chamier, Romain F. Laine, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hern\u00e1ndez-p\u00e9rez, Pieta Mattila, Eleni Karinou, S\u00e9amus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. ZeroCostDL4Mic: an open platform to use Deep-Learning in Microscopy. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.03.20.000133",
        "doi": "https://doi.org/10.1101/2020.03.20.000133"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "Data Augmentation",
        "ZeroCostDL4Mic",
        "Augmentor"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "Notebook Preview"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/platybrowser/platybrowser/3711f1c26e5db8c38c3faff4cccb3110560e3c67/segmentation/cells/UNet3DPlatyCellProbs.model",
      "id": "UNet3DPlatyCellProbs",
      "source": "mmpb.segmentation.network.models.UNetAnisotropic",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/platybrowser/platybrowser/releases/download/1.0.0/UNet3DPlatyCellProbs.model.zip",
      "name": "3D UNet Platynereis Cell Segmentation (Probabilities)",
      "description": "A 3d U-Net trained to predict the cell boundaries in a EM volume of a 6 day old Platynereis.",
      "cite": [
        {
          "text": "Vergara, Hernando M. et al. Whole-body integration of gene expression and single-cell morphology. BioRxiv 2020.\"",
          "doi": "https://doi.org/10.1101/2020.02.26.961037"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "EM",
        "segmentation",
        "unet3d",
        "platynereis",
        "pytorch",
        "ilastik",
        "cell membrane"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_raw.png",
        "ilastik_pred.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/wolny/pytorch-3dunet/37f186c80f4d64b1dab5d165d8c2aae15b5aede1/bioimage-io/UNet3DArabidopsisOvules.model",
      "id": "UNet3DArabidopsisOvules",
      "source": "pytorch3dunet.unet3d.model.UNet3D",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/wolny/pytorch-3dunet/releases/download/1.2.6/UNet3DArabidopsisOvules.model.zip",
      "name": "3D UNet Arabidopsis Ovules",
      "description": "A 3d U-Net trained to predict the cell boundaries in confocal stacks of Arabidopsis ovules. Voxel size: (0.235, 0.150, 0.150) microns ZYX",
      "cite": [
        {
          "text": "Wolny, Adrian et al. Accurate and Versatile 3D Segmentation of Plant Tissues at Cellular Resolution. BioRxiv 2020.",
          "doi": "https://doi.org/10.1101/2020.01.17.910562"
        }
      ],
      "authors": [
        "Adrian Wolny;@bioimage-io"
      ],
      "documentation": "README.md",
      "tags": [
        "arabidopsis",
        "plant tissue",
        "ovuls",
        "segmentation",
        "unet3d",
        "pytorch",
        "ilastik",
        "cell membrane"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "ilastik_4.png",
        "ilastik_5.png",
        "ilastik_6.png",
        "ilastik_7.png",
        "ilastik_8.png"
      ]
    },
    {
      "type": "model",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/pytorch-bioimage-io/v0.1.1/specs/models/unet2d/nuclei_broad",
      "id": "UNet2DNucleiBroad",
      "source": "pybio.torch.models.unet.UNet2d",
      "links": [
        "Ilastik"
      ],
      "download_url": "https://github.com/bioimage-io/pytorch-bioimage-io/releases/download/v0.1.1/UNet2DNucleiBroad.model.zip",
      "name": "2D UNet Nuclei Broad",
      "description": "A 2d U-Net pretrained on broad nucleus dataset.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Constantin Pape;@bioimage-io",
        "Fynn Beuttenm\u00fcller"
      ],
      "documentation": "UNet2DNucleiBroad.md",
      "tags": [
        "nucleus-segmentation",
        "unet2d",
        "pytorch",
        "ilastik"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "cover0.png"
      ]
    },
    {
      "type": "notebook",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "unet-pancreaticcellsegmentation",
      "name": "2D U-Net for binary segmentation",
      "description": "Easy example to define a 2D U-Net for segmentation with Keras and import it into DeepImageJ format",
      "cite": {
        "text": "Falk, T., Mai, D., Bensch, R. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods 16, 67\u201370 (2019).",
        "doi": "https://doi.org/10.1038/s41592-018-0261-2"
      },
      "authors": [
        "Ignacio Arganda-Carreras and DeepImageJ"
      ],
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/notebook_intro.png",
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/usecase.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/deepimagej/models/blob/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
        }
      ],
      "documentation": "https://github.com/miura/NEUBIAS_AnalystSchool2020/tree/master/Ignacio",
      "tags": [
        "DeepImageJ",
        "segmentation",
        "UNet",
        "deepimagej"
      ],
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
    }
  ]
}