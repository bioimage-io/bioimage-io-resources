{
  "id": "bioimage.io",
  "name": "BioImage.IO",
  "tags": [],
  "logo": "\ud83e\udd92",
  "icon": "\ud83e\udd92",
  "splash_title": "BioImage Model Zoo",
  "splash_subtitle": "Advanced AI models in one-click",
  "splash_feature_list": [
    "Integrate with Fiji, Ilastik, ImJoy",
    "Try model instantly with BioEngine",
    "Contribute your models via Github",
    "Link models to datasets and applications"
  ],
  "explore_button_text": "Start Exploring",
  "background_image": "static/img/zoo-background.svg",
  "resource_types": [
    "model",
    "application",
    "notebook",
    "dataset"
  ],
  "default_type": "model",
  "url_root": "https://raw.githubusercontent.com/bioimage-io/bioimage-io-models/main",
  "collections": [
    {
      "id": "ilastik",
      "name": "ilastik",
      "tags": [
        "ilastik"
      ],
      "logo": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/main/image/ilastik-fist-icon.png",
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/main/image/ilastik-fist-icon.png",
      "splash_title": "ilastik",
      "splash_subtitle": "the interactive learning and segmentation toolkit",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model",
      "url_root": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/main"
    },
    {
      "id": "zero",
      "name": "ZeroCostDL4Mic",
      "version": "1.7.1",
      "tags": [
        "ZeroCostDL4Mic"
      ],
      "logo": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "icon": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostLogo.png",
      "splash_title": "ZeroCostDL4Mic",
      "splash_subtitle": "A Google Colab based no-cost toolbox to explore Deep-Learning in Microscopy",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application",
        "dataset"
      ],
      "default_type": "application",
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "imjoy",
      "name": "ImJoy",
      "tags": [
        "imjoy"
      ],
      "logo": "https://imjoy.io/static/img/imjoy-icon.svg",
      "icon": "https://imjoy.io/static/img/imjoy-icon.svg",
      "splash_title": "ImJoy",
      "splash_subtitle": "Deep Learning Made Easy!",
      "splash_feature_list": [
        "Minimal and flexible plugin powered web application",
        "Server-less progressive web application with offline support",
        "Rich and interactive user interface powered by web technologies"
      ],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "notebook",
        "application"
      ],
      "default_type": "application",
      "url_root": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master"
    },
    {
      "id": "hpa",
      "name": "HPA",
      "tags": [
        "hpa"
      ],
      "logo": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "about_url": "https://www.proteinatlas.org/",
      "splash_title": "The Human Protein Atlas",
      "splash_subtitle": null,
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "application"
      ],
      "default_type": "model"
    },
    {
      "id": "fiji",
      "name": "Fiji",
      "tags": [
        "fiji"
      ],
      "logo": "https://fiji.sc/site/logo.png",
      "icon": "https://fiji.sc/site/logo.png",
      "splash_title": "Fiji",
      "splash_subtitle": "Fiji is just ImageJ",
      "splash_feature_list": [],
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook"
      ],
      "url_root": "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master"
    },
    {
      "id": "deepimagej",
      "name": "DeepImageJ",
      "tags": [
        "deepimagej"
      ],
      "logo": "https://raw.githubusercontent.com/deepimagej/models/master/logos/logo.png",
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "splash_title": "deepImageJ",
      "splash_subtitle": "A user-friendly plugin to run deep learning models in ImageJ",
      "splash_feature_list": null,
      "explore_button_text": "Start Exploring",
      "background_image": "static/img/zoo-background.svg",
      "resource_types": [
        "model",
        "notebook",
        "application"
      ],
      "url_root": "https://raw.githubusercontent.com/deepimagej/models/master"
    }
  ],
  "resources": [
    {
      "id": "imjoy/vizarr",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/vizarr.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/hms-dbmi/vizarr/main/assets/logo.png",
      "name": "vizarr",
      "version": "0.0.1",
      "api_version": "0.1.8",
      "description": "A minimal, purely client-side program for viewing Zarr-based images with Viv & ImJoy",
      "requirements": [],
      "dependencies": [],
      "env": "",
      "tags": [
        "bioengine"
      ],
      "documentation": "https://raw.githubusercontent.com/hms-dbmi/vizarr/master/README.md",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "zero/Notebook_pix2pix_2D_ZeroCostDL4Mic",
      "name": "pix2pix (2D) - ZeroCostDL4Mic",
      "description": "pix2pix is a deep-learning method that can be used to translate one type of images into another. While pix2pix can potentially be used for any type of image-to-image translation, we demonstrate that it can be used to predict a fluorescent image from another fluorescent image. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "pix2pix",
        "2D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/pix2pix_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_pix2pix_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "ilastik/ilastik",
      "type": "application",
      "source": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/main/src/ilastik-app.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/ilastik/bioimage-io-models/master/image/ilastik-fist-icon.png",
      "name": "ilastik",
      "version": "0.2.1",
      "api_version": "0.1.7",
      "description": "Export BioImage.IO model packages for ilastik from a model description file",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.5.22/vue.min.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css",
        "https://static.imjoy.io/js/UZIP.js",
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/4.0.0/js-yaml.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/axios/0.19.2/axios.min.js"
      ],
      "dependencies": [],
      "env": "",
      "tags": [
        "bioengine",
        "software"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "deepimagej/deepimagej",
      "type": "application",
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/src/deepimagej-app.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "name": "deepimagej",
      "version": "2.1.10",
      "api_version": "0.1.0",
      "description": "Export BioImage.IO model packages for deepImageJ from a model description file",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.5.22/vue.min.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css",
        "https://static.imjoy.io/js/UZIP.js",
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/4.0.0/js-yaml.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/axios/0.19.2/axios.min.js"
      ],
      "dependencies": [],
      "env": "",
      "tags": [
        "bioengine",
        "software"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "zero/Notebook_YOLOv2_ZeroCostDL4Mic",
      "name": "YOLOv2 - ZeroCostDL4Mic",
      "description": "YOLOv2 is an object detection network developed by Redmon & Farhadi, which identifies objects in images and draws bounding boxes around them.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/YOLOv2_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "YOLOv2",
        "notebook",
        "object detection"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/YOLOv2_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_YOLOv2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_U-Net_3D_ZeroCostDL4Mic_DeepImageJ",
      "name": "U-Net (3D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "The 3D U-Net was first introduced by \u00c7i\u00e7ek et al for learning dense volumetric segmentations from sparsely annotated ground-truth data building upon the original U-Net architecture by Ronneberger et al. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via deepImageJ.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the deepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/U-Net_3D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "3D",
        "notebook",
        "U-Net",
        "deepimagej",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/BioImage.io%20notebooks/U-Net_3D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_U-Net_3D_ZeroCostDL4Mic",
      "name": "U-Net (3D) - ZeroCostDL4Mic",
      "description": "The 3D U-Net was first introduced by \u00c7i\u00e7ek et al for learning dense volumetric segmentations from sparsely annotated ground-truth data building upon the original U-Net architecture by Ronneberger et al. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Daniel Krentzel and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "3D",
        "notebook",
        "U-Net",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_U-Net_2D_ZeroCostDL4Mic_DeepImageJ",
      "name": "U-Net (2D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "U-Net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-Net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via deepImageJ.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the deepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/U-Net_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "2D",
        "notebook",
        "U-Net",
        "deepimagej",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/BioImage.io%20notebooks/U-Net_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_U-Net_2D_ZeroCostDL4Mic",
      "name": "U-Net (2D) - ZeroCostDL4Mic",
      "description": "U-Net is an encoder-decoder architecture originally used for image segmentation. The first half of the U-Net architecture is a downsampling convolutional neural network which acts as a feature extractor from input images. The other half upsamples these results and restores an image by combining results from downsampling with the upsampled images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "2D",
        "notebook",
        "U-Net",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/U-Net_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_StarDist_3D_ZeroCostDL4Mic",
      "name": "StarDist (3D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 3D (xyz) images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "3D",
        "StarDist",
        "notebook",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_StarDist_2D_ZeroCostDL4Mic_DeepImageJ",
      "name": "StarDist (2D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 2D (xy) images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via deepImageJ and StarDist plugins for ImageJ.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the deepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/StarDist_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "StarDist",
        "notebook",
        "2D",
        "deepimagej",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/BioImage.io%20notebooks/StarDist_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_StarDist_2D_ZeroCostDL4Mic",
      "name": "StarDist (2D) - ZeroCostDL4Mic",
      "description": "StarDist is a deep-learning method that can be used to segment cell nuclei in 2D (xy) single images or in stacks (xyz). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "StarDist",
        "notebook",
        "2D",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/StarDist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
        "zero/Dataset_StarDist_Fluo_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_SplineDist_2D_ZeroCostDL4Mic",
      "name": "SplineDist (2D) - ZeroCostDL4Mic",
      "description": "SplineDist is a neural network inspired by StarDist, capable of performing image instance segmentation. Unlike StarDist, SplineDist uses cubic splines to describe the contour of each object and therefore can potentially segment objects of any shapes. This version is only for 2D dataset. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Romain F. Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/SplineDist_overlay_cropped.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/SplineDist_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "SplineDist",
        "notebook",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/SplineDist_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
        "zero/Dataset_StarDist_Fluo_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "deepimagej/EVsTEMsegmentationFRUNet",
      "type": "application",
      "name": "Small extracellular vesicle instance segmentation (FRU-Net)",
      "description": "Ready to use notebook for the segmentation of small extrcaellular vesicles in transmission electron microscopy (TEM) images. The notebook is optimized to use it in Google Colaboratory. It will download the original code and dataset, and make the inference connecting with Google's GPU.",
      "cite": [
        {
          "text": "G\u00f3mez-de-Mariscal, E. et al., Deep-Learning-Based Segmentation of SmallExtracellular Vesicles in Transmission Electron Microscopy Images Scientific Reports, (2019)",
          "doi": "https://doi.org/10.1038/s41598-019-49431-3"
        }
      ],
      "authors": [
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Martin Ma\u0161ka, Anna Kotrbov\u00e1",
        "Vendula Posp\u00edchalov\u00e1",
        "Pavel Matula",
        "Arrate Mu\u00f1oz-Barrutia"
      ],
      "covers": [
        "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-019-49431-3/MediaObjects/41598_2019_49431_Fig1_HTML.png",
        "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation/frunet_sev.jpg"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/BIIG-UC3M/FRU-Net-TEM-segmentation/blob/main/FRUnet_TEM_Exosomes_sEV.ipynb"
        }
      ],
      "documentation": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "tags": [
        "TEM",
        "google colab",
        "extracellular vesicles",
        "workflow",
        "pipeline",
        "notebook",
        "model inference",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/BIIG-UC3M/FRU-Net-TEM-segmentation/master/FRUnet_TEM_Exosomes_sEV.ipynb",
      "links": [
        "deepimagej/FRUNet2DsEVSegmentation"
      ]
    },
    {
      "id": "deepimagej/smlm-deepimagej",
      "type": "application",
      "name": "SMLM-superresolution",
      "description": "Single molecule localization microscopy (SMLM) processing using deepImageJ and ThunderSTORM in an ImageJ macro.",
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/workflows/smlm_deepstorm/cover.png"
      ],
      "download_url": "https://raw.githubusercontent.com/deepimagej/imagej-macros/master/DeepSTORM4stacksThunderSTORM.ijm",
      "source": "https://raw.githubusercontent.com/deepimagej/imagej-macros/master/DeepSTORM4stacksThunderSTORM.ijm",
      "cite": [
        {
          "text": "Lucas von Chamier et al., Nature Communications 2021",
          "doi": "https://doi.org/10.1038/s41467-021-22518-0"
        },
        {
          "text": "G\u00f3mez de Mariscal et al. bioRxiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        },
        {
          "text": "M. Ovesn\u00fd, et al., Bioinformatics 2014",
          "doi": "https://doi.org/10.1093/bioinformatics/btu202"
        }
      ],
      "authors": [
        "DeepImageJ team, UC3M, EPFL"
      ],
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "documentation": "https://raw.githubusercontent.com/deepimagej/models/master/workflows/smlm_deepstorm/README.md",
      "tags": [
        "superresolution",
        "thunderstorm",
        "workflow",
        "pipeline",
        "macro",
        "deepimagej",
        "smlm",
        "deepstorm"
      ]
    },
    {
      "id": "zero/Notebook_RetinaNet_ZeroCostDL4Mic",
      "name": "RetinaNet - ZeroCostDL4Mic",
      "description": "RetinaNet is a is an object detection network, which identifies objects in images and draws bounding boxes around them.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Erlantz Calvo, Ignacio Arganda-Carreras and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/RetinaNet_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "RetinaNet",
        "notebook",
        "object detection"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/RetinaNet_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_YOLOv2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_RCAN_3D_ZeroCostDL4Mic",
      "name": "RCAN (3D) - ZeroCostDL4Mic",
      "description": "RCAN is a neural network capable of image restoration from corrupted bio-images. The network allows image denoising and resolution improvement in 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/3D-RCAN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "CARE",
        "3D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/3D-RCAN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_CARE_3D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Quality_Control_ZeroCostDL4Mic",
      "name": "Quality Control - ZeroCostDL4Mic",
      "description": "This notebooks enable to perform error mapping and quality metrics estimation.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Tools/Quality_Control_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "Quality Control",
        "notebook",
        "ZeroCostDL4Mic"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Tools/Quality_Control_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook Preview",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/nbpreview/master/notebook-preview.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/asset/preview-icon-small.png",
      "name": "Notebook Preview",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "Previewing Jupyter notebook without a Jupyter server",
      "tags": [
        "jupyter",
        "imjoy",
        "notebook",
        "bioengine"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "imjoy/Notebook Preview",
      "type": "application",
      "source": "https://raw.githubusercontent.com/bioimage-io/nbpreview/master/notebook-preview.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/asset/preview-icon-small.png",
      "name": "Notebook Preview",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "Previewing Jupyter notebook without a Jupyter server",
      "tags": [
        "jupyter",
        "imjoy",
        "notebook",
        "bioengine"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "zero/Notebook_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) - ZeroCostDL4Mic",
      "description": "Noise2Void 2D is deep-learning method that can be used to denoise 2D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "2D",
        "Noise2VOID",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_Noise2Void_2D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2VOID (3D) - ZeroCostDL4Mic",
      "description": "Noise2VOID 3D is deep-learning method that can be used to denoise 3D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "Noise2Void",
        "3D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Noise2Void_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_Noise2Void_3D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_MaskRCNN_ZeroCostDL4Mic",
      "name": "MaskRCNN - ZeroCostDL4Mic",
      "description": "MaskRCNN is a is an object detection and segmentation network, which identifies objects in images and draws bounding boxes around them.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/MaskRCNN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "MaskRCNN",
        "notebook",
        "object detection"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/MaskRCNN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_fnet_3D_ZeroCostDL4Mic",
      "name": "Label-free Prediction - fnet - (3D) ZeroCostDL4Mic",
      "description": "Label-free Prediction (fnet) is a neural network used to infer the features of cellular structures from brightfield or EM images without coloured labels. The network is trained using paired training images from the same field of view, imaged in a label-free (e.g. brightfield) and labelled condition (e.g. fluorescent protein). When trained, this allows the user to identify certain structures from brightfield images alone. The performance of fnet may depend significantly on the structure at hand. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/fnet_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "3D",
        "notebook",
        "labelling",
        "fnet"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/fnet_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_fnet_3D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_fnet_2D_ZeroCostDL4Mic",
      "name": "Label-free Prediction - fnet - (2D) ZeroCostDL4Mic",
      "description": "Label-free Prediction (fnet) is a neural network used to infer the features of cellular structures from brightfield or EM images without coloured labels. The network is trained using paired training images from the same field of view, imaged in a label-free (e.g. brightfield) and labelled condition (e.g. fluorescent protein). When trained, this allows the user to identify certain structures from brightfield images alone. The performance of fnet may depend significantly on the structure at hand. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/fnet_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "2D",
        "notebook",
        "labelling",
        "fnet"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/fnet_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Interactive_Segmentation_Kaibu_2D_ZeroCostDL4Mic",
      "name": "Interactive Segmentation - Kaibu (2D) - ZeroCostDL4Mic",
      "description": "Interactive Segmentation using Kaibu and Cellpose.",
      "cite": {
        "text": "Ouyang W, Le T, Xu H and Lundberg E. Interactive biomedical segmentation tool powered by deep learning and ImJoy [version 1; peer review: 1 approved, 1 approved with reservations]. F1000Research 2021, 10:142 (https://doi.org/10.12688/f1000research.50798.1)",
        "doi": "https://doi.org/10.12688/f1000research.50798.1"
      },
      "authors": [
        "Romain Laine, Wei Ouyang and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/ZeroCostDL4Mic_Interactive_annotations_Cellpose.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "Cellpose",
        "ZeroCostDL4Mic",
        "Segmentation",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/ZeroCostDL4Mic_Interactive_annotations_Cellpose.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "imjoy/ImageJ.JS",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/imagej-js.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/imjoy-team/imagej.js/master/src/assets/icons/chrome/chrome-installprocess-128-128.png",
      "name": "ImageJ.JS",
      "version": "0.1.6",
      "api_version": "0.2.3",
      "description": "ImageJ running in the browser",
      "tags": [
        "bioengine",
        "software"
      ],
      "documentation": "https://raw.githubusercontent.com/imjoy-team/imagej.js/master/README.md",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "imjoy/ImJoyFiddle",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/imjoy-fiddle.imjoy.html",
      "passive": false,
      "name": "ImJoyFiddle",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "ImJoyFiddle -- a playground for ImJoy plugins",
      "tags": [
        "bioengine"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "imjoy/ImJoy",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/imjoy-app.imjoy.html",
      "passive": false,
      "icon": "https://imjoy.io/static/img/imjoy-icon.png",
      "name": "ImJoy",
      "version": "0.1.6",
      "api_version": "0.2.3",
      "description": "ImJoy: Supercharging interactivity and scalability!",
      "tags": [
        "imjoy",
        "bioengine",
        "software"
      ],
      "documentation": "https://raw.githubusercontent.com/imjoy-team/ImJoy/master/README.md",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "hpa/HPA-Classification",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/imjoy-plugins/master/repository/HPA-Classification.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/apps/hpa-logo.gif",
      "name": "HPA-Classification",
      "version": "0.2.0",
      "api_version": "0.1.2",
      "description": "ShuffleNetV2 for HPA.",
      "requirements": [
        "https://cdn.jsdelivr.net/npm/apexcharts",
        "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs",
        "https://cdn.jsdelivr.net/npm/simpleheat@0.4.0/simpleheat.min.js",
        "https://cdn.jsdelivr.net/gh/photopea/UTIF.js@4f1b10cb09e244cfd4f9631245d2231537148be7/UTIF.js"
      ],
      "dependencies": [
        "https://raw.githubusercontent.com/imjoy-team/example-plugins/master/imjoy-plugins/HPA-Image-Selection.imjoy.html"
      ],
      "env": null,
      "tags": [
        "bioengine"
      ],
      "documentation": null,
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ],
      "badges": [],
      "authors": []
    },
    {
      "id": "fiji/Fiji",
      "name": "Fiji",
      "description": "Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis.",
      "source": "https://fiji.sc/",
      "cite": {
        "text": "Schindelin, J., Arganda-Carreras, I., Frise, E. et al. Fiji: an open-source platform for biological-image analysis. Nat Methods 9, 676\u2013682 (2012).",
        "doi": "https://doi.org/10.1038/nmeth.2019"
      },
      "authors": [
        "Fiji community"
      ],
      "icon": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/Fiji-icon.png",
      "documentation": "https://fiji.sc/",
      "git_repo": "https://github.com/fiji/fiji,",
      "passive": true,
      "tags": [
        "fiji",
        "software"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Detectron2_ZeroCostDL4Mic",
      "name": "Detectron2 - ZeroCostDL4Mic",
      "description": "Detectron2 is an object detection network developed by Facebook AI Research, which identifies objects in images and draws bounding boxes around them.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/Detectron2_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "notebook",
        "object detection",
        "Detectron2"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/Detectron2_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_YOLOv2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_DenoiSeg_2D_ZeroCostDL4Mic",
      "name": "DenoiSeg (2D) - ZeroCostDL4Mic",
      "description": "DenoiSeg 2D is deep-learning method that can be used to jointly denoise and segment 2D microscopy images. The benefits of using DenoiSeg (compared to other Deep Learning-based segmentation methods) are more prononced when only a few annotated images are available. However, the denoising part requires many images to perform well. All the noisy images don't need to be labeled to train DenoiSeg. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "CycleGAN",
        "2D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/DenoiSeg_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "deepimagej/deepimagej-web",
      "type": "application",
      "name": "DeepImageJ",
      "description": "DeepImageJ is a user-friendly plugin that enables the use of pre-trained deep learning models in ImageJ and Fiji.",
      "source": "https://deepimagej.github.io/deepimagej/index.html",
      "cite": {
        "text": "G\u00f3mez-de-Mariscal, E., Garc\u00eda-L\u00f3pez-de-Haro, C., Ouyang, W., Donati, L., Lundberg E., Unser, M., Mu\u00f1oz-Barrutia, A. and Sage, D. DeepImageJ: A user-friendly plugin to run deep learning models in ImageJ, BioRxiv, 2019",
        "doi": "https://doi.org/10.1101/799270"
      },
      "authors": [
        "DeepImageJ team"
      ],
      "icon": "https://raw.githubusercontent.com/deepimagej/models/master/logos/icon.png",
      "documentation": "https://deepimagej.github.io/deepimagej/index.html",
      "git_repo": "https://github.com/deepimagej/deepimagej-plugin",
      "passive": true,
      "tags": [
        "deepimagej",
        "software"
      ],
      "config": {
        "supported_weight_formats": [
          "tensorflow_saved_model_bundle"
        ]
      }
    },
    {
      "id": "zero/Notebook_Deep-STORM_2D_ZeroCostDL4Mic_DeepImageJ",
      "name": "Deep-STORM (2D) - ZeroCostDL4Mic - DeepImageJ",
      "description": "Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by Nehme et al. in Optica. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these. Networks trained in this notebook can be used in Fiji via deepImageJ and ThunderSTORM plugin.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Estibaliz G\u00f3mez de Mariscal and the deepImageJ and the ZeroCostDL4Mic teams"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/Deep-STORM_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "deepImageJ",
        "Deep-STORM",
        "2D",
        "notebook",
        "DeepImageJ"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/BioImage.io%20notebooks/Deep-STORM_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_Deep-STORM_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Deep-STORM_2D_ZeroCostDL4Mic",
      "name": "Deep-STORM (2D) - ZeroCostDL4Mic",
      "description": "Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by Nehme et al. in Optica. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension). Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Romain Laine and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "Deep-STORM",
        "2D",
        "notebook",
        "labelling"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Deep-STORM_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_Deep-STORM_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_DecoNoising_2D_ZeroCostDL4Mic",
      "name": "DecoNoising (2D) - ZeroCostDL4Mic",
      "description": "DecoNoising 2D is deep-learning method that can be used to denoise 2D microscopy images. By running this notebook, you can train your own network and denoise your images. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/DecoNoising_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "2D",
        "notebook",
        "DecoNoising"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/DecoNoising_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_Noise2Void_2D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_DRMIME_ZeroCostDL4Mic",
      "name": "DRMIME - ZeroCostDL4Mic",
      "description": "DRMIME is an network that can be used to register microscopy images (affine and perspective registration).",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/DRMIME_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "image registration",
        "DRMIME",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/DRMIME_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_CycleGAN_2D_ZeroCostDL4Mic",
      "name": "CycleGAN (2D) - ZeroCostDL4Mic",
      "description": "CycleGAN is a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples (ie transform a horse into zebra or apples into oranges). While CycleGAN can potentially be used for any type of image-to-image translation, we illustrate that it can be used to predict what a fluorescent label would look like when imaged using another imaging modalities. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "CycleGAN",
        "2D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CycleGAN_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_CycleGAN_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_Cellpose_2D_ZeroCostDL4Mic",
      "name": "Cellpose (2D) - ZeroCostDL4Mic",
      "description": "Cellpose is a generalist algorithm for cellular segmentation.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/Cellpose_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "Cellpose",
        "ZeroCostDL4Mic",
        "Segmentation",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Beta%20notebooks/Cellpose_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
        "zero/Dataset_StarDist_Fluo_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield_ZeroCostDL4Mic",
        "zero/Dataset_StarDist_brightfield2_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "imjoy/CellPose",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/cellpose.imjoy.html",
      "passive": false,
      "name": "CellPose",
      "version": "0.1.0",
      "api_version": "0.2.3",
      "description": "a generalist algorithm for cellular segmentation",
      "tags": [
        "bioengine"
      ],
      "documentation": "https://cellpose.readthedocs.io/en/latest/",
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "zero/Notebook_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "CARE",
        "3D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_3D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_CARE_3D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "zero/Notebook_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) - ZeroCostDL4Mic",
      "description": "CARE is a neural network capable of image restoration from corrupted bio-images, first published in 2018 by Weigert et al. in Nature Methods. The network allows image denoising and resolution improvement in 2D and 3D images, in a supervised training manner. The function of the network is essentially determined by the set of images provided in the training dataset. For instance, if noisy images are provided as input and high signal-to-noise ratio images are provided as targets, the network will perform denoising. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Lucas von Chamier and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "denoising",
        "CARE",
        "2D",
        "notebook"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview",
        "zero/Dataset_CARE_2D_ZeroCostDL4Mic"
      ],
      "type": "application"
    },
    {
      "id": "imjoy/BioImageIO-Packager",
      "type": "application",
      "source": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/src/bioimageio-packager.imjoy.html",
      "passive": false,
      "icon": "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/asset/download-icon.png",
      "name": "BioImageIO-Packager",
      "version": "0.3.1",
      "api_version": "0.1.7",
      "description": "Exporting BioImage.IO model packages from the model description file",
      "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.5.22/vue.min.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css",
        "https://cdnjs.cloudflare.com/ajax/libs/js-yaml/4.0.0/js-yaml.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/axios/0.19.2/axios.min.js",
        "https://static.imjoy.io/js/UZIP.js"
      ],
      "dependencies": [],
      "env": "",
      "tags": [
        "bioengine"
      ],
      "documentation": null,
      "covers": [],
      "badges": [],
      "authors": []
    },
    {
      "id": "zero/Notebook_Augmentor_ZeroCostDL4Mic",
      "name": "Augmentor - ZeroCostDL4Mic",
      "description": "Augmentor is a data augmentation library. Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation can be especially valuable when training dataset need to be manually labelled. Note - visit the ZeroCostDL4Mic wiki to check the original publications this network is based on and make sure you cite these.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet and the ZeroCostDL4Mic Team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/oeway/ZeroCostDL4Mic/master/Wiki_files/ZeroCostDL4Mic_SuppVideo2_Analysis_of_example_data.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb"
        }
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "tags": [
        "colab",
        "ZeroCostDL4Mic",
        "Data Augmentation",
        "notebook",
        "Augmentor"
      ],
      "source": "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "links": [
        "zero/Notebook Preview"
      ],
      "type": "application"
    },
    {
      "id": "deepimagej/unet-pancreaticcellsegmentation",
      "type": "application",
      "name": "2D U-Net for binary segmentation",
      "description": "Easy example to define a 2D U-Net for segmentation with Keras and import it into DeepImageJ format",
      "cite": [
        {
          "text": "Falk, T., Mai, D., Bensch, R. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods 16, 67\u201370 (2019).",
          "doi": "https://doi.org/10.1038/s41592-018-0261-2"
        }
      ],
      "authors": [
        "Ignacio Arganda-Carreras",
        "DeepImageJ team"
      ],
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/notebook_intro.png",
        "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/usecase.png"
      ],
      "badges": [
        {
          "label": "Open in Colab",
          "icon": "https://colab.research.google.com/assets/colab-badge.svg",
          "url": "https://colab.research.google.com/github/deepimagej/models/blob/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb"
        }
      ],
      "documentation": "https://github.com/miura/NEUBIAS_AnalystSchool2020/tree/master/Ignacio",
      "tags": [
        "training",
        "unet",
        "notebook",
        "deepimagej",
        "cell segmentation",
        "segmentation"
      ],
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/U_Net_PhC_C2DL_PSC_segmentation.ipynb",
      "links": [
        "deepimagej/UNet2DPancreaticSegmentation"
      ]
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_pix2pix_ZeroCostDL4Mic",
      "name": "pix2pix example training and test dataset - ZeroCostDL4Mic",
      "description": "Paired microscopy images (fluorescence) of lifeact-RFP and sir-DNA",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941889",
      "tags": [
        "ZeroCostDL4Mic",
        "pix2pix"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/paired-image_translation.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941889"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_YOLOv2_ZeroCostDL4Mic",
      "name": "YoloV2 example training and test dataset - ZeroCostDL4Mic",
      "description": "2D grayscale .png images with corresponding bounding box annotations in .xml  PASCAL Voc format.",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet",
        "Lucas von Chamier"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941908",
      "tags": [
        "ZeroCostDL4Mic",
        "YOLOv2"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941908"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_txred_super-resolution/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_txred_super-resolution",
      "id": "deepimagej/WidefieldTxredSuperResolution",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_txred_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Widefield Super-resolution (GAN - TxRed)",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "GAN",
        "Fluorescence microscopy",
        "deepimagej"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": "CC BY 4.0",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_fitc_super-resolution/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_fitc_super-resolution",
      "id": "deepimagej/WidefieldFitcSuperResolution",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_fitc_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Widefield Super-resolution (GAN - FITC)",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "GAN",
        "Fluorescence microscopy",
        "deepimagej"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": "CC BY 4.0",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_dapi_super-resolution/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/widefield_dapi_super-resolution",
      "id": "deepimagej/WidefieldDapiSuperResolution",
      "download_url": "https://zenodo.org/record/4290871/files/widefield_dapi_superresolution.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Widefield Super-resolution (GAN - DAPI)",
      "description": "A trained GAN to transform diffraction-limited input images into super-resolved ones.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Methods 2019",
          "doi": "https://doi.org/10.1038/s41592-018-0239-0"
        }
      ],
      "authors": [
        "Hongda Wang",
        "Yair Rivenson",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "Super resolution",
        "GAN",
        "Fluorescence microscopy",
        "deepimagej"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": "CC BY 4.0",
      "error": {}
    },
    {
      "type": "model",
      "source": "https://raw.githubusercontent.com/bioimage-io/pytorch-bioimage-io/v0.3.0/specs/models/unet2d_nuclei_broad/UNet2DNucleiBroad.model.yaml",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/pytorch-bioimage-io/v0.3.0/specs/models/unet2d_nuclei_broad",
      "id": "ilastik/UNet2DNucleiBroad",
      "links": [
        "ilastik/Ilastik",
        "imjoy/BioImageIO-Packager"
      ],
      "download_url": "https://github.com/bioimage-io/pytorch-bioimage-io/releases/download/v0.3.0/UNet2DNucleiBroad.model.zip",
      "format_version": "0.3.0",
      "name": "UNet 2D Nuclei Broad",
      "description": "A 2d U-Net trained on the nuclei broad dataset.",
      "authors": [
        "Constantin Pape;@bioimage-io",
        "Fynn Beuttenm\u00fcller"
      ],
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        },
        {
          "text": "2018 Data Science Bowl",
          "url": "https://www.kaggle.com/c/data-science-bowl-2018"
        }
      ],
      "git_repo": "https://github.com/bioimage-io/pytorch-bioimage-io/tree/master/specs/models/unet2d",
      "tags": [
        "dsb2018",
        "nucleus",
        "unet2d",
        "ilastik",
        "segmentation",
        "pytorch"
      ],
      "license": "MIT",
      "documentation": "UNet2DNucleiBroad.md",
      "covers": [],
      "error": {}
    },
    {
      "type": "model",
      "source": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2/UNetDA.model.yaml",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "ilastik/UNetDA",
      "links": [
        "ilastik/Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/UNetDA.model.zip",
      "name": "U-Net DA (Domain Adaptation)",
      "description": "U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "hbp",
        "brain",
        "unet2d",
        "ilastik",
        "sga2",
        "vasculature",
        "pytorch"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/UNetCover.png"
      ],
      "error": {}
    },
    {
      "type": "model",
      "source": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2/2sUNetDA.model.yaml",
      "root_url": "https://raw.githubusercontent.com/subeeshvasu/hbp-DL-seg-codes/0.1.2",
      "id": "ilastik/2sUNetDA",
      "links": [
        "ilastik/Ilastik"
      ],
      "download_url": "https://github.com/subeeshvasu/hbp-DL-seg-codes/releases/download/0.1.2/2sUNetDA.model.zip",
      "name": "Two Steam U-Net DA",
      "description": "Two Steam U-Net trained on brain vasculature segmentation data from Ludovico Silvestri's European Laboratory for Non-linear Spectroscopy (LENS). Two Steam U-Net is used as the segmentation network that takes up the inputs from source and target domain, and generate the respective segmentation results at the output. Two Steam U-Net uses differet encoders to process inputs from source and target, and use a common decoder to generate the respective segmentation outputs. To train the network, cross entropy loss between the prediction and ground truth labels is used for the source data. For the target data, image reconstrcution constraints are enforced on the segmentation outputs. Furthermore, source domain images are translated into the target domain using an adverserial paradigm, to generate auxiliary labelled data for the target domain. The labelled data thus generated are used to establish a supervised loss in the target domain.",
      "cite": [
        {
          "text": "Roger Bermudez et al. A domain-adaptive two-stream U-Net for electron microscopy image segmentation. ISBI 2018.",
          "doi": "https://doi.org/10.1109/ISBI.2018.8363602"
        }
      ],
      "authors": [
        "Roger Bermudez, Vasu Subeesh"
      ],
      "documentation": "documentation/TransferLearningBasedSegmentationWorkflow.md",
      "tags": [
        "hbp",
        "brain",
        "unet2d",
        "ilastik",
        "sga2",
        "vasculature",
        "pytorch"
      ],
      "license": "MIT",
      "format_version": "0.1.0",
      "covers": [
        "documentation/covers/2sUNetCover.png"
      ],
      "error": {}
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_StarDist_2D_ZeroCostDL4Mic_2D",
      "name": "StarDist (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Johanna Jukkala",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3715492",
      "tags": [
        "ZeroCostDL4Mic",
        "segmentation",
        "StarDist",
        "2D"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Stardist_nuclei_masks.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3715492"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./exampleImage.tiff",
          "./resultImage.tiff",
          "./postprocessing.txt",
          "./postprocessingWatershed.txt"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/fru-net_sev_segmentation",
      "id": "deepimagej/FRUNet2DsEVSegmentation",
      "links": [
        "deepimagej/deepimagej",
        "deepimagej/EVsTEMsegmentationFRUNet",
        "imjoy/BioImageIO-Packager"
      ],
      "download_url": "https://zenodo.org/record/4156050/files/deepimagej_fru-net_sev_segmentation.zip",
      "format_version": "0.3.0",
      "name": "Small Extracellular Vesicle TEM Segmentation (Fully Residual U-Net)",
      "description": "DeepImageJ compatible fully residual U-Net trained to segment small extracellular vesicles in 2D TEM images",
      "cite": [
        {
          "text": "G\u00f3mez-de-Mariscal, E. et al., Deep-Learning-Based Segmentation of SmallExtracellular Vesicles in Transmission Electron Microscopy Images Scientific Reports, (2019)",
          "doi": "https://doi.org/10.1038/s41598-019-49431-3"
        }
      ],
      "authors": [
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Martin Ma\u0161ka, Anna Kotrbov\u00e1",
        "Vendula Posp\u00edchalov\u00e1",
        "Pavel Matula",
        "Arrate Mu\u00f1oz-Barrutia"
      ],
      "documentation": "https://cbia.fi.muni.cz/research/segmentation/fru-net.html",
      "covers": [
        "frunet_sev.jpg"
      ],
      "tags": [
        "segmentation",
        "extracellular vesicles",
        "TEM",
        "deepimagej"
      ],
      "license": "BSD-3-Clause",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.ijm",
          "./post.ijm",
          "./0066.tif",
          "./lesion.csv",
          "./0066_mask.png"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/skin_lesion_classification/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/skin_lesion_classification",
      "id": "deepimagej/SkinLesionClassification",
      "download_url": "https://zenodo.org/record/4155785/files/SkinLesions.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej-web"
      ],
      "format_version": "0.3.1",
      "name": "Skin lesions classification",
      "description": "CNN trained to classify the type of imaged skin lesion.",
      "authors": [
        "Carlos Garc\u00eda-L\u00f3pez-de-Haro"
      ],
      "documentation": "https://gist.github.com/esgomezm/7398a83321ae589bafadb0392c7b78ef#file-skin_lession_classification_pytorch_original-ipynb",
      "covers": [
        "cover.jpg"
      ],
      "tags": [
        "classification",
        "skin lesions",
        "deepimagej",
        "deepimagej-beta",
        "pytorch",
        "melanoma"
      ],
      "license": "BSD2",
      "git_repo": "https://gist.github.com/esgomezm/72b584887ca5ed5ed0231c47fff9aa9b#file-skin_lession_classification_pytorch_adapted-ipynb",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./Results.csv",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/defcon_density_map_estimation/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/defcon_density_map_estimation",
      "id": "deepimagej/SMLMDensityMapEstimationDEFCoN",
      "download_url": "https://zenodo.org/record/4608442/files/SMLM_Density%20Map_Estimation_%28DEFCoN%29.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "SMLM Density Map Estimation (DEFCoN)",
      "description": "Density Estimation by Fully Convolutional Networks (DEFCoN) - A fluorescent spot counter for single molecule localization microscopy. DEFCoN was written by Baptiste Ottino as a Masters thesis project under the guidance of Kyle M. Douglass and Suliana Manley in the Laboratory of Experimental Biophysics.",
      "cite": null,
      "authors": [
        "Baptiste Ottino",
        "Kyle M. Douglass",
        "Suliana Manley"
      ],
      "documentation": "https://github.com/LEB-EPFL/DEFCoN-ImageJ/wiki",
      "covers": [
        "cover_image.jpg"
      ],
      "tags": [
        "unet",
        "deepimagej",
        "deepimagej.js"
      ],
      "license": "BSD 3",
      "git_repo": "https://github.com/LEB-EPFL/DEFCoN",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_pancreatic_segmentation",
      "id": "deepimagej/UNet2DPancreaticSegmentation",
      "download_url": "https://github.com/deepimagej/models/releases/download/0.3/PancreaticCellSegmentation.U-Net.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "deepimagej/unet-pancreaticcellsegmentation",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Pancreatic Phase Contrast Cell Segmentation (U-Net)",
      "description": "DeepImageJ compatible U-Net trained to segment phase contrast microscopy images of pancreatic stem cells on a 2D polystyrene substrate.",
      "cite": [
        {
          "text": "G\u00f3mez-de-Mariscal E. et al., biorXiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        },
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.4473"
        },
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        }
      ],
      "authors": [
        "Ignacio Arganda-Carreras",
        "DeepImageJ team"
      ],
      "documentation": "https://github.com/deepimagej/models/u-net_pancreatic_segmentation/U_Net_PhC-C2DL-PSC_segmentation.ipynb",
      "git_repo": "https://github.com/deepimagej/models/u-net_pancreatic_segmentation/",
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "phase contrast",
        "segmentation",
        "pancreatic stem cells",
        "deepimagej"
      ],
      "license": "BSD-2",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tiff",
          "./resultImage.tiff"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/mu-lux_ctc_phc-c2dl-psc/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/mu-lux_ctc_phc-c2dl-psc",
      "id": "deepimagej/MU-Lux_CTC_PhC-C2DL-PSC",
      "download_url": "https://zenodo.org/record/4155785/files/MU-Lux_CTC_PhC-C2DL-PSC.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Pancreatic Cell Phase Contrast Segmentation (DeepWater - CTC submission)",
      "description": "The method combines deep learning with watershed segmentation. For each frame, the convolutional neural network of U-Net shape detects all cells by markers and recognizes the foreground and the background of the frame. Then, the final segmentation is generated by a Marker-Controlled Watershed transformation.",
      "cite": [
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.447"
        },
        {
          "text": "Filip Lux and Petr Matula, arXiv 2020",
          "doi": "https://arxiv.org/abs/2004.01607"
        }
      ],
      "authors": [
        "Filip Lux, Centre for Biomedical Image Analysis, Masaryk University",
        "Petr Matula, Centre for Biomedical Image Analysis, Masaryk University"
      ],
      "git_repo": "https://gitlab.fi.muni.cz/xlux/deepwater",
      "documentation": "http://public.celltrackingchallenge.net/participants/MU-Lux-CZ.pdf",
      "covers": [
        "cover.jpg"
      ],
      "tags": [
        "phase contrast",
        "deepwater",
        "deepimagej",
        "watershed",
        "segmentation",
        "cell tracking challenge"
      ],
      "license": "MIT",
      "error": {}
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_Noise2Void_3D_ZeroCostDL4Mic",
      "name": "Noise2Void (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713326",
      "tags": [
        "Noise2Void",
        "3D",
        "ZeroCostDL4Mic",
        "denoising"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713326"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_Noise2Void_2D_ZeroCostDL4Mic",
      "name": "Noise2Void (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (paxillin-GFP)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Aki Stubb",
        "Guillaume Jacquemet",
        "Johanna Ivaska"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713315",
      "tags": [
        "Noise2Void",
        "2D",
        "ZeroCostDL4Mic",
        "denoising"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/N2V_wiki.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713315"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.ijm",
          "./postprocessing.ijm",
          "./General.jar",
          "./config.ijm",
          "./exampleImage.tif",
          "./MAX_finalMask.tif",
          "./fibroblasts_detection.csv"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/usiigaci/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/usiigaci",
      "id": "deepimagej/Usiigaci",
      "download_url": "https://zenodo.org/record/4155785/files/usiigaci.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.1",
      "name": "NIH/3T3 Fibroblast Phase Contrast Segmentation (Usiigaci-Mask R-CNN)",
      "description": "Trained model given in Usiigaci for instance segmentation of cells in a 2D substrate and phase contrast microscopy images. The model is a Mask R-CNN and the input is preprocessed to get an RGB image from the grayscale image.",
      "cite": [
        {
          "text": "Tsai, H.-F., et al., SoftwareX, 2019",
          "doi": "https://doi.org/10.1016/j.softx.2019.02.007"
        }
      ],
      "authors": [
        "Joanna Gajda",
        "Tyler F.W. Sloan",
        "Hsieh-Fu Tsai",
        "Andrei Rares",
        "Amy Q. Shen"
      ],
      "covers": [
        "cover.jpg"
      ],
      "tags": [
        "phase contrast",
        "deepimagej-beta",
        "fibroblasts",
        "2D",
        "deepimagej",
        "segmentation",
        "maskrcnn",
        "usiigaci"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/oist/Usiigaci",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "weights": {
          "tensorflow_saved_model_bundle": {
            "source": "https://github.com/bioimage-io/fiji-bioimage-io/releases/download/v0.3.1/n2v-sem-demo_savedmodelbundle.zip",
            "sha256": "c55ab5528ec0dd8b3e00062df155ba00aef4971d7eacc3ed2d41db1f25917994",
            "authors": [
              {
                "name": "Deborah Schmidt",
                "affiliation": "MDC Berlin"
              }
            ],
            "tensorflow_version": "1.13.1"
          }
        }
      },
      "source": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/models/n2v-sem-demo/model.yaml",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/fiji-bioimage-io/master/models/n2v-sem-demo",
      "id": "fiji/N2VSEMDemo",
      "links": [
        "fiji/Fiji"
      ],
      "download_url": "https://github.com/bioimage-io/fiji-bioimage-io/releases/download/0.3.1/n2v-sem-demo.zip",
      "name": "N2V SEM Demo",
      "description": "Demo model for denoising trained on a single SEM image with Noise2Void",
      "cite": [
        {
          "text": "Buchholz, T. et al. - Content-aware image restoration for electron microscopy. Methods in Cell Biology, Volume 152 p.277-289, ISSN 0091-679X (2019)",
          "doi": "https://doi.org/10.1016/bs.mch.2019.05.001"
        }
      ],
      "authors": [
        {
          "name": "Deborah Schmidt",
          "affiliation": "MDC Berlin"
        }
      ],
      "documentation": "README.md",
      "covers": [
        "thumbnail.png"
      ],
      "tags": [
        "fiji",
        "n2v",
        "unet2d",
        "denoising"
      ],
      "license": "BSD-3-Clause",
      "format_version": "0.3.2",
      "error": {}
    },
    {
      "type": "dataset",
      "id": "deepimagej/MoNuSeg_digital_pathology_miccai2018",
      "name": "Multi-Organ Nucleus Segmentation Challenge - MICCAI 2018",
      "description": "Labelled images for instance segmentation of cell nuclei in digital pathology datasets (MoNuSeg 2018 Challenge).",
      "cite": {
        "text": "Neeraj Kumar et al. Transactions on medical imaging 2020",
        "doi": "https://doi.org/10.1109/TMI.2019.2947628"
      },
      "authors": [
        "DeepImageJ team, UC3M, EPFL"
      ],
      "documentation": "https://monuseg.grand-challenge.org",
      "tags": [
        "digital pathology",
        "H&E",
        "histology",
        "StarDist",
        "2D",
        "deepimagej",
        "segmentation",
        "nuclei segmentation",
        "pathology"
      ],
      "covers": [
        "https://raw.githubusercontent.com/deepimagej/models/master/datasets/MoNuSeg1.jpg",
        "https://raw.githubusercontent.com/deepimagej/models/master/datasets/MoNuSeg2.jpg",
        "https://raw.githubusercontent.com/deepimagej/models/master/datasets/MoNuSeg3.jpg"
      ],
      "source": "https://monuseg.grand-challenge.org/Data/"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/mt3_virtual_staining/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/mt3_virtual_staining",
      "id": "deepimagej/Mt3VirtualStaining",
      "download_url": "https://zenodo.org/record/4290839/files/mt3_virtual_staining.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Masson\u2019s Trichrome Virtual Staining (GAN)",
      "description": "A trained GAN to transform wide-field autofluorescence images of unlabelled tissue sections into images that are equivalent to the bright-field images of histologically stained versions of the same samples.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Biomedical Engineering, 2019",
          "doi": "https://doi.org/10.1038/s41551-019-0362-y"
        }
      ],
      "authors": [
        "Yair Rivenson",
        "Hongda Wang",
        "Aydogan Ozcan"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "histology",
        "Virtual staining",
        "GAN",
        "deepimagej"
      ],
      "license": "CC BY 4.0",
      "error": {}
    },
    {
      "type": "dataset",
      "id": "imjoy/LuCa-7color",
      "name": "LuCa-7color",
      "description": "Sample PerkinElmer Vectra QPTIFF files (c) PerkinElmer (http://www.perkinelmer.com)",
      "authors": [
        "Perkin Elmer"
      ],
      "tags": [
        "imjoy",
        "OME-TIFF"
      ],
      "license": "CC-BY 4.0",
      "download_url": "https://downloads.openmicroscopy.org/images/Vectra-QPTIFF/perkinelmer/PKI_scans/LuCa-7color_Scan1.qptiff",
      "covers": [
        "https://raw.githubusercontent.com/imjoy-team/bioimage-io-models/master/asset/LuCa-7color_Scan1.png"
      ],
      "links": [
        "imjoy/vizarr"
      ],
      "source": "https://viv-demo.storage.googleapis.com/LuCa-7color_Scan1/data.zarr/0"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_fnet_3D_ZeroCostDL4Mic",
      "name": "Label-free prediction (fnet) example training and test dataset - ZeroCostDL4Mic",
      "description": "Confocal microscopy data (TOM20 labeled with Alexa Fluor 594)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Christoph Spahn"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3748967",
      "tags": [
        "ZeroCostDL4Mic",
        "3D",
        "fnet",
        "labelling"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/Fnet_exemplary_data_mitochondria.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3748967"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/jones_virtual_staining/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/jones_virtual_staining",
      "id": "deepimagej/JonesVirtualStaining",
      "download_url": "https://zenodo.org/record/4290839/files/jones_virtual_staining.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Jones Virtual Staining (GAN)",
      "description": "A trained GAN to transform wide-field autofluorescence images of unlabelled tissue sections into images that are equivalent to the bright-field images of histologically stained versions of the same samples.",
      "cite": [
        {
          "text": "Hongda Wang, Yair Rivenson, et al., Nature Biomedical Engineering, 2019",
          "doi": "https://doi.org/10.1038/s41551-019-0362-y"
        }
      ],
      "authors": [
        "Yair Rivenson",
        "Hongda Wang",
        "Aydogan Ozcan"
      ],
      "covers": [
        "exampleImage.png",
        "resultImage.png"
      ],
      "tags": [
        "histology",
        "Virtual staining",
        "GAN",
        "deepimagej"
      ],
      "documentation": "https://innovate.ee.ucla.edu",
      "license": "CC BY 4.0",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tiff",
          "./Results.csv",
          "./resultImage.tiff"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_hela_segmentation",
      "id": "deepimagej/UNet2DHeLaSegmentation",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "download_url": "https://zenodo.org/record/4155785/files/deepimagej_u-net_hela_segmentation.zip",
      "format_version": "0.3.0",
      "name": "HeLa DIC Cell Segmentation (U-Net)",
      "description": "DeepImageJ compatible U-Net trained to segment Hela cells in 2D phase contrast microscopy images",
      "cite": [
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        },
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.4473"
        }
      ],
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "https://deepimagej.github.io/deepimagej/models_documentation.html",
      "covers": [
        "unet_hela_seg.jpg"
      ],
      "tags": [
        "phase contrast",
        "segmentation",
        "hela cells",
        "deepimagej"
      ],
      "license": "BSD-2",
      "git_repo": "https://github.com/deepimagej/python4deepimagej/tree/master/unet",
      "error": {}
    },
    {
      "type": "model",
      "id": "hpa/HPA-wienerschnitzelgemeinschaft",
      "name": "HPA-wienerschnitzelgemeinschaft",
      "tags": [
        "classification",
        "inception-v3",
        "resnet-50",
        "resnet-18",
        "hpa",
        "airnet-50",
        "airnext-50",
        "resnet-34",
        "resnet-101",
        "preresnet-50",
        "se-resnext-50",
        "inception-resnet-v2",
        "hill-climbing",
        "cbam"
      ],
      "authors": [
        "Shaikat Mahmood Galib",
        "Christof Henkel",
        "Kevin Hwang",
        "Dmytro Poplavskiy",
        "Bojan Tunguz",
        "Russ Wolfinger"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wienerschnitzelgemeinschaft",
      "description": "An ensemble of diverse and highly optimized CNN models, using hill climbing and weighted voting.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wienerschnitzelgemeinschaft/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "4th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-vpp",
      "name": "HPA-vpp",
      "tags": [
        "classification",
        "inception-v3",
        "hpa",
        "multilayer-perceptron",
        "inception-v4",
        "xception"
      ],
      "authors": [
        "Yinzheng Gu",
        "Chuanpeng Li",
        "Jinbin Xie"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/vpp",
      "description": "An ensemble of seven CNN models and a multi-layer perceptron network, using image augmentation, multi scales, weighted sampling and MultiLabelSoftMargin loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/vpp/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "5th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-pudae",
      "name": "HPA-pudae",
      "tags": [
        "classification",
        "inception-v3",
        "hpa",
        "resnet-34",
        "se-resnext-50"
      ],
      "authors": [
        "Park Jinmo"
      ],
      "license": "BSD-2-Clause",
      "git_repo": "https://github.com/CellProfiling/pudae-kaggle-hpa",
      "description": "An ensemble using focal loss, per image normalization and spatial attention.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/pudae-kaggle-hpa/master/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "3rd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-conv",
      "name": "HPA-conv is all you need",
      "tags": [
        "classification",
        "inception-v3",
        "hpa",
        "se-resnext-50",
        "xception"
      ],
      "authors": [
        "Xuan Cao",
        "Runmin Wei",
        "Yuanhao Wu",
        "Xun Zhu"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/conv_is_all_you_need",
      "description": "An ensemble of a cropping window CNN based on Xception, and two conventional CNNs based on SE-ResNext50 and InceptionV3.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/conv_is_all_you_need/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "10th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-bestfitting",
      "name": "HPA-bestfitting",
      "tags": [
        "classification",
        "inception-v3",
        "resnet-50",
        "densenet-121",
        "hpa",
        "resnet-34"
      ],
      "authors": [
        "Shubin Dai"
      ],
      "license": "MIT",
      "cite": null,
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting",
      "description": "A CNN model using focal loss and image augmentation, optimized with Adam optimizer.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "1st",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png",
        "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/bestfitting/src/bestfitting-densenet-diagram.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-WAIR",
      "name": "HPA-WAIR",
      "tags": [
        "scikit-learn",
        "classification",
        "densenet-121",
        "se-resnext-50",
        "hpa",
        "opencv",
        "ibn-densenet-121",
        "densenet-169",
        "xception"
      ],
      "authors": [
        "Jun Lan"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/wair",
      "description": "Seven CNN models, using multiple different architectures, ensembled through averaging.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/wair/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "2nd",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-Random-Walk",
      "name": "HPA-Random Walk",
      "tags": [
        "resnet-18",
        "hpa",
        "attention-gate",
        "classification"
      ],
      "authors": [
        "Zhifeng Gao",
        "Cheng Ju",
        "Xiaohan Yi",
        "Hongdong Zheng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/random_walk",
      "description": "An ensemble model of three different resolutions based on single attention gated network.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/random_walk/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "38th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-One-More",
      "name": "HPA-One More Layer Of Stacking",
      "tags": [
        "classification",
        "hpa",
        "lightgbm",
        "bn-inception",
        "se-resnext-50",
        "inception-v4",
        "xception"
      ],
      "authors": [
        "Dmitry Buslov",
        "Sergei Fironov",
        "Alexander Kiselev",
        "Dmytro Panchenko"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/one_more_layer_of_stacking",
      "description": "14 CNN models ensembled via LightGBM stacking, optimized with Wadam, using focal and LSEP loss.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/one_more_layer_of_stacking/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "8th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "id": "hpa/HPA-NTU_MiRA",
      "name": "HPA-NTU_MiRA",
      "tags": [
        "hpa",
        "classification",
        "resnet-34"
      ],
      "authors": [
        "Kuan-Lun Tseng"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/CellProfiling/HPA-competition-solutions/tree/master/ntu_mira",
      "description": "A CNN with large input size (1024 \u2a09 1024 px) using fixed batch-normalization and data distillation.",
      "documentation": "https://raw.githubusercontent.com/CellProfiling/HPA-competition-solutions/master/ntu_mira/README.md",
      "badges": [
        {
          "label": "HPA Competition",
          "ext": "16th",
          "url": "https://www.kaggle.com/c/human-protein-atlas-image-classification/leaderboard"
        }
      ],
      "covers": [
        "https://raw.githubusercontent.com/CellProfiling/HPA-model-zoo/master/hpa_challenge_header.png"
      ]
    },
    {
      "type": "model",
      "source": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/models/HPAShuffleNetV2/HPAShuffleNetV2.model.yaml",
      "root_url": "https://raw.githubusercontent.com/bioimage-io/tfjs-bioimage-io/master/models/HPAShuffleNetV2",
      "id": "hpa/HPAShuffleNetV2",
      "links": [
        "hpa/HPA-Classification"
      ],
      "name": "HPA ShuffleNetV2",
      "description": "A light-weight model for HPA image classification competition",
      "cite": null,
      "authors": [
        "Moshe Livne",
        "Wei OUYANG"
      ],
      "documentation": "HPAShuffleNetV2.md",
      "tags": [
        "imjoy",
        "classification",
        "tensorflow.js",
        "shufflenet",
        "hpa"
      ],
      "git_repo": "https://github.com/CellProfiling/HPA-Special-Prize",
      "badges": [
        {
          "url": "https://imjoy.io",
          "icon": "https://imjoy.io/static/badge/powered-by-imjoy-badge.svg",
          "label": "Powered by ImJoy"
        }
      ],
      "license": "Apache 2.0",
      "format_version": "0.1.0",
      "covers": [
        "https://imjoy-team.github.io/imjoy-plugins/hpa-classification/hpa-classification-cover.gif"
      ],
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.txt",
          "./postprocessing.txt",
          "./exampleImage.tiff",
          "./Results.csv",
          "./resultImage.tiff"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_glioblastoma_segmentation/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/u-net_glioblastoma_segmentation",
      "id": "deepimagej/UNet2DGlioblastomaSegmentation",
      "download_url": "https://zenodo.org/record/4155785/files/deepimagej_u-net_glioblastoma_segmentation.zip",
      "links": [
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Glioblastoma Phase Contrast Cell Segmentation (U-Net)",
      "description": "DeepImageJ compatible U-Net trained to segment 2D phase contrast microscopy images of glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate.",
      "cite": [
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        },
        {
          "text": "Ulman V. et al., Nature Methods 2017",
          "doi": "https://doi.org/10.1038/nmeth.4473"
        }
      ],
      "authors": [
        "Jo\u00e3o Soares Lopes"
      ],
      "documentation": "README.md",
      "covers": [
        "cover_image.jpg"
      ],
      "tags": [
        "glioblastoma cells",
        "segmentation",
        "phase contrast",
        "deepimagej"
      ],
      "license": "BSD-2",
      "git_repo": "https://github.com/deepimagej/python4deepimagej/tree/master/unet",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.ijm",
          "./postprocessing_LocalMaximaSMLM.ijm",
          "./exampleImage.tiff",
          "./Localizations_resultImage_max.csv",
          "./resultImage.tiff",
          "./DeepSTORM4stacksThunderSTORM.ijm",
          "./postprocessing_AveragedMaximaSMLM.ijm"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/deepstorm_zerocostdl4mic/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/deepstorm_zerocostdl4mic",
      "id": "deepimagej/DeepSTORMZeroCostDL4Mic",
      "download_url": "https://zenodo.org/record/4155785/files/DeepImageJ_DeepSTORM_ZeroCostDL4Mic.zip",
      "links": [
        "zero/Notebook_Deep-STORM_2D_ZeroCostDL4Mic_DeepImageJ",
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "Glial Cell SMLM (DeepSTORM - ZeroCostDL4Mic)",
      "description": "A trained Deep-STORM model for image reconstruction from high-density single-molecule localization microscopy (SMLM).",
      "cite": [
        {
          "text": "Nehme E. et al., Optica 2018",
          "doi": "https://doi.org/10.1364/OPTICA.5.000458"
        },
        {
          "text": "Lucas von Chamier et al. biorXiv 2020",
          "doi": "https://doi.org/10.1101/2020.03.20.000133"
        },
        {
          "text": "G\u00f3mez de Mariscal et al. bioRxiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        }
      ],
      "authors": [
        "ZeroCostDL4Mic team",
        "DeepImageJ team"
      ],
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "documentation": "https://deepimagej.github.io/deepimagej",
      "covers": [
        "input.png",
        "zoom.png"
      ],
      "tags": [
        "zerocostdl4mic",
        "deepimagej",
        "image reconstruction",
        "SMLM",
        "super-resolution"
      ],
      "license": "MIT",
      "error": {}
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_Deep-STORM_ZeroCostDL4Mic",
      "name": "Deep-STORM training and example dataset - ZeroCostDL4Mic",
      "description": "Time-series of simulated, randomly distributed single-molecule localization (SMLM) data (Training dataset). Experimental time-series dSTORM acquisition of Glial cells stained with phalloidin for actin (Example dataset).",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Christophe Leterrier",
        "Romain F. Laine"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3959089",
      "tags": [
        "ZeroCostDL4Mic",
        "SMLM",
        "2D",
        "Deep-STORM"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/TrainingDataset_ShowOff_v3.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3959089"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_CycleGAN_ZeroCostDL4Mic",
      "name": "CycleGAN example training and test dataset - ZeroCostDL4Mic",
      "description": "Unpaired microscopy images (fluorescence) of microtubules (Spinning-disk and SRRF reconstructed images)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "CycleGAN",
        "ZeroCostDL4Mic"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/unpaired-image_translation.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3941884"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_StarDist_brightfield2_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 3 - Flow chamber dataset",
      "description": "Paired brightfield images of cancer cells and corresponding masks",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Gautier Follain",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingflo_trackmate.png"
      ],
      "source": "https://zenodo.org/record/4034939"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_StarDist_brightfield_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 2 - T cell dataset",
      "description": "Paired brightfield images of migrating T cells and corresponding masks",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Nathan H. Roy",
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingTcells_trackmate.png"
      ],
      "source": "https://zenodo.org/record/4034929"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_StarDist_Fluo_ZeroCostDL4Mic",
      "name": "Combining StarDist and TrackMate example 1 - Breast cancer cell dataset",
      "description": "Fluorescence microscopy of Nuclei (SiR-DNA) and masks obtained via manual segmentation",
      "cite": {
        "text": "Elnaz Fazeli, Nathan H. Roy, Gautier Follain, Romain F. Laine, Lucas von Chamier, Pekka E. H\u00e4nninen, John E. Eriksson, Jean-Yves Tinevez, Guillaume Jacquemet. Automated cell tracking using StarDist and TrackMate. bioRxiv, 2020. DOI: https://doi.org/10.1101/2020.09.22.306233",
        "doi": "https://doi.org/10.1101/2020.09.22.306233"
      },
      "authors": [
        "Guillaume Jacquemet"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3941884",
      "tags": [
        "ZeroCostDL4Mic",
        "StarDist"
      ],
      "covers": [
        "https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Wiki_files/StarDist_trainingfluo_trackmate.png"
      ],
      "source": "https://zenodo.org/record/4034976"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_CARE_3D_ZeroCostDL4Mic",
      "name": "CARE (3D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713337",
      "tags": [
        "ZeroCostDL4Mic",
        "3D",
        "denoising",
        "CARE"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713337"
    },
    {
      "type": "dataset",
      "id": "zero/Dataset_CARE_2D_ZeroCostDL4Mic",
      "name": "CARE (2D) example training and test dataset - ZeroCostDL4Mic",
      "description": "Fluorescence microscopy (Lifeact-RFP)",
      "cite": {
        "text": "von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). https://doi.org/10.1038/s41467-021-22518-0",
        "doi": "https://doi.org/10.1038/s41467-021-22518-0"
      },
      "authors": [
        "Guillaume Jacqueme"
      ],
      "documentation": "https://doi.org/10.5281/zenodo.3713330",
      "tags": [
        "ZeroCostDL4Mic",
        "2D",
        "denoising",
        "CARE"
      ],
      "covers": [
        "https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/CARE_wiki.png"
      ],
      "source": "https://doi.org/10.5281/zenodo.3713330"
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.ijm",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/3du-net_zerocostdl4mic/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/3du-net_zerocostdl4mic",
      "id": "deepimagej/3DUNetZeroCostDL4Mic",
      "download_url": "https://zenodo.org/record/4155785/files/3DUNet_ZeroCostDL4Mic.bioimage.io.model.zip",
      "links": [
        "deepimagej/deepimagej",
        "zero/Notebook_U-Net_3D_ZeroCostDL4Mic_DeepImageJ",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "3D U-Net - ZeroCostDL4Mic",
      "description": "3D U-Net trained using ZeroCostDL4Mic notebooks to segment mitochondria in Transmission Electron Microscopy (TEM) data.",
      "cite": [
        {
          "text": "Lucas von Chamier et al. bioRxiv 2020",
          "doi": "https://doi.org/10.1101/2020.03.20.000133"
        },
        {
          "text": "\u00d6zg\u00fcn \u00c7i\u00e7ek et al., MICCAI 2016",
          "doi": "https://doi.org/10.1007/978-3-319-46723-8_49"
        }
      ],
      "authors": [
        "DeepImageJ team"
      ],
      "documentation": "https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki",
      "covers": [
        "exampleImage.gif"
      ],
      "tags": [
        "ZeroCostDL4Mic",
        "TEM",
        "deepimagej-beta",
        "deepimagej",
        "3DUNet",
        "mitochondria",
        "segmentation"
      ],
      "license": "MIT",
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/U-Net_3D_ZeroCostDL4Mic.ipynb",
      "error": {}
    },
    {
      "type": "model",
      "attachments": {
        "files": [
          "./preprocessing.ijm",
          "./postprocessing.ijm",
          "./exampleImage.tif",
          "./resultImage.tif"
        ]
      },
      "source": "https://raw.githubusercontent.com/deepimagej/models/master/2du-net_zerocostdl4mic/model.yaml",
      "root_url": "https://raw.githubusercontent.com/deepimagej/models/master/2du-net_zerocostdl4mic",
      "id": "deepimagej/2DUNetZeroCostDL4Mic",
      "download_url": "https://zenodo.org/record/4155785/files/DeepImageJ_2D%20UNet_ZeroCostDL4Mic.zip",
      "links": [
        "zero/Notebook_U-Net_2D_ZeroCostDL4Mic_DeepImageJ",
        "deepimagej/deepimagej",
        "imjoy/BioImageIO-Packager"
      ],
      "format_version": "0.3.0",
      "name": "2D UNet - ZeroCostDL4Mic",
      "description": "2D U-Net trained for binary segmentation using the EM images of neuronal membranes and segmentation masks from the ISBI segmentation challenge 2012.",
      "cite": [
        {
          "text": "Ronneberger O. et al., MICCAI 2015",
          "doi": "https://doi.org/10.1007/978-3-319-24574-4_28"
        },
        {
          "text": "Lucas von Chamier et al. bioRxiv 2020",
          "doi": "https://doi.org/10.1101/2020.03.20.000133"
        },
        {
          "text": "G\u00f3mez de Mariscal et al. bioRxiv 2019",
          "doi": "https://doi.org/10.1101/799270"
        }
      ],
      "authors": [
        "ZeroCostDL4Mic team",
        "DeepImageJ team"
      ],
      "git_repo": "https://github.com/HenriquesLab/ZeroCostDL4Mic",
      "documentation": "https://deepimagej.github.io/deepimagej",
      "covers": [
        "cover.png"
      ],
      "tags": [
        "segmentation",
        "unet",
        "zerocostdl4mic",
        "deepimagej"
      ],
      "license": "MIT",
      "error": {}
    }
  ]
}